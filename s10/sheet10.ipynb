{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bb2639bd101f12969138b7f5411be81",
     "grade": false,
     "grade_id": "cell-header01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Computer Vision (Winter Term 2020/21) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland, Ludwig Schallner, Artem Petrov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1368af6b0cbc02cbbade156285affb0b",
     "grade": false,
     "grade_id": "cell-header02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 10: Model Based Recognition / Motion¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18b0f2cc9ad1c048578f51dbd74176b4",
     "grade": false,
     "grade_id": "cell-header03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Saturday, January 23, 2021**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e16bcd51d5f0b0b1ecf39947f5f8c33c",
     "grade": false,
     "grade_id": "cell-433af82c3ad3533b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Understanding the Wireframe-Model [5 points]\n",
    "\n",
    "This exercise addresses the matching procedure described on (CV-12 slides 9-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec8810ccd9e0f3035177865758f89c77",
     "grade": false,
     "grade_id": "cell-0b3896d77598ffeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain in your own words the functions on slide  (CV-12 slide 9). Also explain when and why it may make sense to use $m$ instead of $m'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c1492c64761670139be82a1732109a3",
     "grade": true,
     "grade_id": "cell-bacb3a7a7e5fc45d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "From the initial pose, the wire frame model is iteratively adapted to the image based on gradients.  \n",
    "\n",
    "$m$: magnitude, $\\beta$: orientation, $g(x, y)$: image  \n",
    "\n",
    "**$x$-gradient**: $\\Delta_x g = g(x+1, y) - g(x-1, y) \\rightarrow$ for a fixed $y$, it's the difference between the pixels to the left and to the right  \n",
    "**$y$-gradient**: $\\Delta_y g = g(x, y+1) - g(x, y-1) \\rightarrow$ for a fixed $x$, it's the difference between the pixels to above and below  \n",
    "**gradient magnitude**: $m'(x, y) = \\sqrt{\\Delta_x g^2 + \\Delta_y g^2}$  \n",
    "**orientation:** Use the inverse tangent: $\\beta(x, y) = arctan(\\frac{\\Delta_y g}{\\Delta_x g})$  \n",
    "\n",
    "For the gradient magnitude, there's an alternative computation which is thresholded:  \n",
    "$m(x, y) = \\Theta(m'(x, y) - T)$ (only takes magnitudes that are sufficiently large)  \n",
    "\n",
    "**TODO**: When and Why $m$ instead of $m'$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42b70588ca2d7ccf6fb3c6f8acce6847",
     "grade": false,
     "grade_id": "cell-4ca9815f8438dacd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Explain the fitness score $E_{S_i}$ and $E_l$. What do the arrows (CV-13 slide 11), e.g. $\\beta_j$ and $S_j$, indicate? What is the idea of $G(d)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43868721aae715b10c3463c316ca960",
     "grade": true,
     "grade_id": "cell-56353d87181c759c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$E_{S_i}$ - Fitness score for pixel $S_i$ in search rectangle $R$: $E_{S_i} = |m(x, y) \\cdot (\\sin (\\beta(x, y) - \\alpha))|$\n",
    "- fitness must be proportional to the gradient magnitude $m(x, y)$ (only strong gradients should contribute, not noise)\n",
    "- $\\beta$ (gradient orientation), $\\alpha$ (direction of the line) - we take their difference\n",
    "    - if the difference is $0$, it's bad, so we take the $\\sin$ and get a fitness score of $0$\n",
    "    - it would be perfect if they are perpendicular ($90°$), then the $\\sin$ is $1$\n",
    "\n",
    "\n",
    "$E_l$ - Total fitness score of line segment $l$: $E_l = \\sum_{S_i \\in R} E_{S_i} \\cdot G_{\\mu = 0, \\sigma = W} (d_i)$\n",
    "- sum of fitness scores over all pixels $S_i$ of search rectangle $R$\n",
    "- weighted by Gaussian distance function\n",
    "- why the search rectangle?\n",
    "    - what we want is that pixels are on the line segments and that these belong to the edge\n",
    "    - but we could not guide the search then\n",
    "    - we have to look to the left and right of the line segment\n",
    "    - pixels to the left and right contribute a little, but not as much as the ones perfectly on the line\n",
    "    - that's why we have this Gaussian weighting ($\\sigma$ is chosen appropriately based on the width of the rectangle)\n",
    "    - if we have a certain fitness, the fitness score $E_{S_j}$ of pixel $S_j$ can be improved by moving the line segment in the direction $\\beta_j$\n",
    "- the Gaussian distance function is there to guide the search in the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "852e9b932deb5d8537170dd15cbf8cda",
     "grade": false,
     "grade_id": "cell-b47bc76dc83866f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Explain the goal of EDA (Estimation of Distribution Algorithm) and how it is performed in the context of the matching procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1513750a454a516765b672c10b935fe2",
     "grade": true,
     "grade_id": "cell-a415fef187bc15ad",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The goal is to optimize the $15$ shape, position, and pose parameters to fit the local gradients.\n",
    "\n",
    "We start with a generation of individuals (several wireframe models) which have different position, pose, and shape parameters.  \n",
    "This generation needs to be optimized. Each of the individuals is just a point in the 15D space,\n",
    "so we start with a point cloud in the 15D space.\n",
    "\n",
    "Now, the next generation will be sampled from this distribution and with a random process new individuals will be produced.  \n",
    "We project those into the plane and compute the fitness scores.  \n",
    "Finally, the selection comes into play: We choose a certain number of individuals with highest fitness scores  \n",
    "and only from these compute the next generation (survival of the fittest).\n",
    "\n",
    "Generation of new individuals: Gaussian density estimation of remaining point cloud.  \n",
    "We use the density estimation as a biased\n",
    "random number generator to produce new points. The parents give a bias to the offspring,  \n",
    "but the offspring does not perfectly confirm to that. There is a chance for the next generation to be different (better).\n",
    "\n",
    "The process works Iteratively until the specified stop criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6d5742b5d5e4b83a7ebbb8cbad9722a",
     "grade": false,
     "grade_id": "cell-1d40dc9fdb0bf40d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Histogram of Oriented Gradients (HOG) [5 points]\n",
    "\n",
    "The *Histogram of Oriented Gradients (HOG)* applied in the initial step of the wireframe matching procedure is also applied in other computer vision algorithms, especially in the context of object recognition. This exercise will examine this tool in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "268e7cee9ef6d16a873895cacc9d2916",
     "grade": false,
     "grade_id": "cell-97dab94bb51c3f56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain the idea of the histogram of oriented gradients. How can it be applied to analyze images? Think how this idea may be used to recognize objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90efdd67cb71238626b3e7e112ce1c58",
     "grade": true,
     "grade_id": "cell-d8b32d15ead04253",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The technique counts occurrences of gradient orientations in an image, it's basically a feature descriptor that can be used for object detection.  \n",
    "\n",
    "Gradients ($x$ and $y$ derivatives) of an image are useful because the magnitude of gradients is large around edges and corners  \n",
    "(regions of abrupt intensity changes) and edges and corners provide a lot more information about object shape than flat regions.  \n",
    "\n",
    "To calculate a HOG descriptor, we need to first calculate the horizontal and vertical gradients.  \n",
    "This is easily achieved by filtering the image with the horizontal and vertical Sobel filters $[-1, 0, 1]$ and $[-1, 0, 1]^T$.  \n",
    "Afterwards, we can get the magnitude and direction of the gradient using the formulas from the previous task.\n",
    "\n",
    "At every pixel, the gradient has a magnitude and a direction. The next step is to create a histogram of gradients.  \n",
    "A bin is selected based on the direction, and the vote (the value that goes into the bin) is selected based on the magnitude. \n",
    "\n",
    "Finally, we have all the information we need in a kind of compressed way. Such a histogram of oriented gradients can be  \n",
    "further processed and become a feature vector that can for example be used in classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a6ab67ea8b799e2b785411feba153e8",
     "grade": false,
     "grade_id": "cell-10c3d543e2b137f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** The Scikit-image library provides the function [`hog`](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) that can compute histograms of oriented gradients and offers also an option to construct a visualization. Run the following code cell and then describe your observations in the text cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "image = imageio.imread('./images/truck.jpeg')\n",
    "image = resize(image,(700,1000),preserve_range=True).astype(np.uint8)\n",
    "\n",
    "fd, hog_image = hog(image,feature_vector=False,visualize=True, multichannel=True)\n",
    "\n",
    "# Display the result\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8, 12))\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()\n",
    "\n",
    "bars = ax3.bar(np.linspace(0,180,fd.shape[-1]),fd[0,0,0,0],width=(180/fd.shape[-1]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def on_press(event):\n",
    "    \"\"\"Mouse button press event handler\n",
    "    Args:\n",
    "    event: The mouse event\n",
    "    \"\"\"\n",
    "    x, y = int(event.xdata)//8, int(event.ydata)//8\n",
    "    \n",
    "    cell_x = x - fd.shape[1] if x >= fd.shape[1] else 0\n",
    "    x = min(x,fd.shape[1]-1)\n",
    "    cell_y = y - fd.shape[0] if y >= fd.shape[0] else 0\n",
    "    y = min(y,fd.shape[0]-1)\n",
    "    ax3.clear()\n",
    "    ax3.set_title(f\"x={x} [{cell_x}], y={y} [{cell_y}], {fd.shape}\")\n",
    "    ax3.bar(np.linspace(0,180,fd.shape[-1]),fd[y,x,cell_y,cell_x],width=(180/fd.shape[-1]))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_press)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f0985debc63d80893465cac49e6abe4",
     "grade": true,
     "grade_id": "cell-31e8b8c6a964c63a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f72ca67ad2347cbb541307d699a10455",
     "grade": false,
     "grade_id": "cell-882e8894af9db5c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Implement your own version of the histogram of oriented gradients function. You may proceed in the following steps:\n",
    "1. Compute the gradient image and determine magnitude and direction of gradients.\n",
    "2. Divide the image into cells and compute a weighted histogram for each cell.\n",
    "3. Use the function to [`plt.quiver`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html) to display your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0279e41a4184d2f91eb413030fff3198",
     "grade": true,
     "grade_id": "cell-12fe45a690d01a6c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: compute gradients\n",
    "\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "def image_gradients(image):\n",
    "    \"\"\"Compute gradient magnitudes and directions for a given image.\n",
    "    \n",
    "    Input:\n",
    "        image: an numpy.ndarray of shape (HEIGHT, WIDTH)\n",
    "    Result:\n",
    "        magnitude, direction: two numpy.ndarrays of the same shape as image,\n",
    "        holding gradient magnitudes and directions, respectively.\n",
    "    \"\"\"\n",
    "    # Hint: you may ues the sobel function to obtain x- and y- gradients\n",
    "    magnitude = np.zeros_like(image, dtype=np.float32)\n",
    "    direction = np.zeros_like(image, dtype=np.float32)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return magnitude, direction\n",
    "\n",
    "image = rgb2gray(imageio.imread('./images/car.png').astype(np.uint8))\n",
    "magnitude, direction = image_gradients(image)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.gray()\n",
    "plt.subplot(1,2,1); plt.title(\"Image\")\n",
    "plt.imshow(image)\n",
    "plt.subplot(1,2,2); plt.title(\"Gradient magnitude\")\n",
    "plt.imshow(magnitude)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cdf5bd58825845cb710e2719f132d18",
     "grade": true,
     "grade_id": "cell-c8e5a429fe49742e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the histograms\n",
    "\n",
    "def histogram_of_oriented_gradients(image, cell_size=(16,16), bins=9):\n",
    "    \"\"\"Compute histograms of oriented gradients for an image.\n",
    "    Input:\n",
    "        image: image: an numpy.ndarray of shape (HEIGHT, WIDTH)\n",
    "        cell_size: the size of individual cells into which the image is divided\n",
    "        bins: the number of bins per histogram\n",
    "    Result:\n",
    "        An np.ndarray of shape (CELL_ROWS, CELL_COLUMNS, BINS) containing\n",
    "        the histograms for the individual cells\n",
    "    \"\"\"\n",
    "    # Hint: you may use np.histogram() here\n",
    "    rows, columns = image.shape[0]//cell_size[0], image.shape[1]//cell_size[1]\n",
    "    hog = np.zeros((rows, columns, bins))\n",
    "    magnitude, direction = image_gradients(image)\n",
    "    # YOUR CODE HERE\n",
    "    return hog\n",
    "\n",
    "hog = histogram_of_oriented_gradients(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c27a0924da214eec9aa9af6e3ad9b26",
     "grade": true,
     "grade_id": "cell-02ebf1639aa8ee00",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: display your results\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cell_size=(8,8)\n",
    "hog = histogram_of_oriented_gradients(image, cell_size=cell_size)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.gca().invert_yaxis()\n",
    "# YOUR CODE HERE\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a57fd865376fa0b52fd5281c794f081",
     "grade": false,
     "grade_id": "cell-aaf0af689d8940da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Understanding Optical Flow [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "841329f57b8ed41fcb7656e4320f99de",
     "grade": false,
     "grade_id": "cell-optical-flow-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is *optical flow*? Explain the concept on an intuitive level. Contrast it with physical movement and visual displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31a424138c8cb886dd7a769098957e8b",
     "grade": true,
     "grade_id": "cell-optical-flow-a1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Optical flow is about understanding how things are moving in an image or a sequence of images (at the pixel level).  \n",
    "It's essentially a vector that describes the detected motion for every pixel, not necessarily reflecting true motion in the real world.\n",
    "\n",
    "The true displacement can not be detected, e.g. due to the aperture problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23900192508f9afbcc92b7711cf00912",
     "grade": false,
     "grade_id": "cell-optical-flow-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Explain the optical flow equation. What is that line depicted on (CV-13 slide 21)? What do different points on this line have in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d85e9fcd3c480623dd1818d8116c99b",
     "grade": true,
     "grade_id": "cell-optical-flow-a2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$v_x \\cdot g_x + v_y \\cdot g_y + g_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c9725c553ca586212e1f416fe6ebb06",
     "grade": false,
     "grade_id": "cell-optical-flow-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the aperture problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30f93b32d8aa2192545bfaf3f8d6b2a2",
     "grade": true,
     "grade_id": "cell-optical-flow-a3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The aperture problem arises when you have not enough information to accurately figure out how the object is moving (e.g. when you can't see the edges).  \n",
    "A typical example for the aperture problem ist the 'barber pole illusion'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4188a15d05cd97615e1289259628376",
     "grade": false,
     "grade_id": "cell-optical-flow-q4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Execute the following demo. Vary the value for `direction` (valid values are `None`, `'horizontal'`, and `'vertical'`). What do you see? Discuss your observations in the text field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Choose one of the three directions:\n",
    "direction = 'horizontal'\n",
    "#direction = 'vertical'\n",
    "#direction = None\n",
    "\n",
    "image = np.ndarray((100,100,3), dtype=np.uint8)\n",
    "\n",
    "def barbers_pole(image, time=0, direction=None):\n",
    "    image[:,:] = (255,255,255)\n",
    "    height, width = image.shape[:2]\n",
    "    strip = width//4\n",
    "    xx, yy = np.meshgrid(range(strip), range(height))\n",
    "    image[yy,(xx + yy + time) % width] = (255,0,0)\n",
    "    image[yy,(xx + yy + time + 2*strip)% width] = (0,0,255)\n",
    "    if direction == 'vertical':\n",
    "        image[:,:strip] = 0\n",
    "        image[:,3*strip:] = 0\n",
    "    elif direction == 'horizontal':\n",
    "        image[:strip] = 0\n",
    "        image[3*strip:] = 0\n",
    "\n",
    "barbers_pole(image)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "for i in range(500):\n",
    "    ax.set_title(f\"frame={i}\")\n",
    "    barbers_pole(image, i, direction=direction)\n",
    "    im.set_data(image)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1df6f275d7cc84ec63c238a1c43098a7",
     "grade": true,
     "grade_id": "cell-optical-flow-a4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3dbb3a993a745da9da281bae66b49958",
     "grade": false,
     "grade_id": "cell-motion-impl",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Implementing Optical Flow [6 Points]\n",
    "\n",
    "This exercise aims at obtaining the optical flow from a video. The following two cells provide code to create simple demo videos. You may use either of these cells, but be aware that the second video may result in large movies, requiring heavy computation (you may reduce duration or frame size). Hence you are recommended to start developing your code using the first video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_movie1(size=(20, 20), duration=50):\n",
    "    \"\"\"Create a small movie showing a moving dot.\n",
    "    \n",
    "    Result:\n",
    "        a numpy.ndarray of shape (FRAMES, HEIGHT, WIDTH)\n",
    "    \"\"\"\n",
    "    foreground = 0\n",
    "    background = 1\n",
    "\n",
    "    movie = np.ones(shape=(duration,)+size, dtype=np.float32) * background\n",
    "\n",
    "    for t in range(duration):\n",
    "        position = (t%size[0], t%size[1])\n",
    "        movie[t, position[0], position[1]] = foreground\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def make_movie2(duration = 400, show=False):\n",
    "    \"\"\"A simple movie created from the example from the lecture slides.\n",
    "\n",
    "    Result:\n",
    "        a numpy.ndarray of shape (FRAMES, HEIGHT, WIDTH)\n",
    "    \"\"\"\n",
    "    image = imageio.imread('images/movie.png')\n",
    "    car = image[200:300,340:480].copy()\n",
    "    image[200:300,340:480] = 255\n",
    "\n",
    "    if show:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.title(f\"{image.shape}\")\n",
    "        im = ax.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "    movie = np.ndarray((duration,)+image.shape[:2], dtype=np.float32)\n",
    "\n",
    "    for t in range(duration):\n",
    "        # do not clear everything, just adapt the artists\n",
    "        frame = image.copy()\n",
    "        x, y = t, 200\n",
    "        box = frame[y:y+car.shape[0],x:x+car.shape[1]] \n",
    "        box[car!=255] = car[car!=255]\n",
    "        movie[t] = rgb2gray(frame)\n",
    "        if show:\n",
    "            ax.set_title(f\"Creating movie frame={t+1}/{duration}\")\n",
    "            im.set_data(frame)\n",
    "            fig.canvas.draw()\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and display the video\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def show_movie(movie, delay=0.1):\n",
    "    \"\"\"Show a movie using matplotlib.\n",
    "    Arguments:\n",
    "        movie: a numpy.ndarray of shape (FRAMES, HEIGHT, WIDTH)\n",
    "        delay: time to sleep between frames (in seconds)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.axis('off')\n",
    "    plt.gray()\n",
    "    im = ax.imshow(movie[0])\n",
    "\n",
    "    for t, frame in enumerate(movie):\n",
    "        ax.set_title(f\"frame={t}\")\n",
    "        im.set_data(frame)\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(0.1)\n",
    "    plt.close()\n",
    "\n",
    "movie = make_movie1()\n",
    "show_movie(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "653f7f0e3d62c5ba40baee3d344b95b2",
     "grade": false,
     "grade_id": "cell-67cc8a3745a954db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain the idea of the Horn-Schunck algorithm. What are the *intensity constancy assumption* and the *spatial motion constancy assumption* and how do they enter into the algorithm? Explain the the ideas and the individual steps for computing the optical flow. Then provide an implementation in the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a1d38cdb4d1fd9bfefc499121d59fe7",
     "grade": true,
     "grade_id": "cell-d9984e7310ffce8d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The Horn-Schunck algorithm is a basic method for optical flow detection from an image sequence.\n",
    "It's a global method which introduces a global constraint of smoothness to solve the aperture problem.\n",
    "\n",
    "**Intensity constancy assumption**  \n",
    "If during the (small) time $t$ between frames $t$ and $\\Delta t+1$ a pixel moves from $(x,y)$ to $(x+ \\Delta x, y + \\Delta y)$, then its intensity remains constant: $g(x, y, t) = g(x + \\Delta x, y + \\Delta y, t + \\Delta t)$.\n",
    "That means, we assume there is no change in intensity due to changes of illumination etc.\n",
    "\n",
    "**Spatial motion constancy assumption**  \n",
    "Adjacent pixels have the same optical flow. This holds for most pixels, because the area of moving edges is usually smaller than the area of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4787af2dc0507aa4a23249c8b40aab85",
     "grade": true,
     "grade_id": "cell-41751c752b07d8e1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def horn_schunck(movie, iterations=10, λ=0.5):\n",
    "    \"\"\"The Horn-Schunck algorithm. \n",
    "    Input:\n",
    "         movie: a numpy.ndarray of shape (FRAMES, HEIGHT, WIDTH)\n",
    "         iterations: number of iterations to run\n",
    "         λ: the lambda parameter of the algorithm (0<λ<=1).\n",
    "    Output:\n",
    "         v_x, v_y: two movies of the same shape as `movie`, \n",
    "                   providing the x and y component of the optical flow\n",
    "    \"\"\"\n",
    "    v_x = np.zeros_like(movie)\n",
    "    v_y = np.zeros_like(movie)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return v_x, v_y\n",
    "\n",
    "\n",
    "movie = make_movie1()\n",
    "v_x, v_y = horn_schunck(movie)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,4))\n",
    "plt.gray()\n",
    "\n",
    "ax1.set_title(\"Movie\")\n",
    "im_frame = ax1.imshow(movie[0], cmap='gray')\n",
    "\n",
    "ax2.set_title(\"Optical flow\")\n",
    "ax2.set_aspect('equal')\n",
    "ax2.invert_yaxis()\n",
    "flow = ax2.quiver(np.arange(movie.shape[2]), np.arange(movie.shape[1]), -v_x[0], v_y[0], scale=2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for t, frame in enumerate(movie):\n",
    "    fig.suptitle(f\"frame={t}\")\n",
    "    im_frame.set_data(movie[t])    \n",
    "    flow.set_UVC(-v_x[t], v_y[t])\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(0.1)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ba0c068cc879f3a14ab9210cdccb957",
     "grade": false,
     "grade_id": "cell-a53200b0563b3d7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** What is the idea of the Lucas-Kanade algorithm? Point out differences to the Horn-Schunck algorithm. Explain why the problem of overdetermination does occur and how the algorithm deals with that problem? Then implement the algorithm in the code cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e447a873f0a0dffeb466978692d3437",
     "grade": true,
     "grade_id": "cell-7d51decc29e5b203",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "148544a4cc261286c158d14d245064eb",
     "grade": true,
     "grade_id": "cell-37c97308b77c5a41",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def lucas_kanade(movie):\n",
    "    \"\"\"The Lucas-Kanade algorithm. \n",
    "    Input:\n",
    "         movie: a numpy.ndarray of shape (FRAMES, HEIGHT, WIDTH)\n",
    "    Output:\n",
    "         v_x, v_y: two movies of the same shape as `movie`, \n",
    "                   providing the x and y component of the optical flow\n",
    "    \"\"\"\n",
    "    v_x = np.zeros_like(movie)\n",
    "    v_y = np.zeros_like(movie)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return v_x, v_y\n",
    "\n",
    "\n",
    "movie = make_movie1()\n",
    "v_x, v_y = lucas_kanade(movie)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2 ,figsize=(8,8))\n",
    "plt.gray()\n",
    "\n",
    "ax1.set_title(\"Movie\")\n",
    "im_frame = ax1.imshow(movie[0], cmap='gray')\n",
    "\n",
    "ax2.set_title(\"Optical flow\")\n",
    "ax2.set_aspect('equal')\n",
    "ax2.invert_yaxis()\n",
    "flow = ax2.quiver(np.arange(movie.shape[2]), np.arange(movie.shape[1]), -v_x[0], v_y[0], scale=2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for t, frame in enumerate(movie):\n",
    "    fig.suptitle(f\"frame={t}\")\n",
    "    im_frame.set_data(movie[t])    \n",
    "    flow.set_UVC(-v_x[t], v_y[t])\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(0.1)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('cv': conda)",
   "language": "python",
   "name": "python38664bitcvcondace24c6b5e63f40158ccc45b6baeafab5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}