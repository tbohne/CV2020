{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c562c88b8fa15ece85ca9e7fc43de9b",
     "grade": false,
     "grade_id": "cell-60aa580d9d920dba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr체ck University - Computer Vision (Winter Term 2020/21) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland, Ludwig Schallner, Artem Petrov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd1e8ac94ba7799a848aba2c58e771dc",
     "grade": false,
     "grade_id": "cell-d9e6599459b0ba4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Recap I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d70ad337e01e2f7482d85df9960423a1",
     "grade": false,
     "grade_id": "cell-917ded279d27040b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This sheet is a recap of the first half of the term. Neither do you have to present it to your tutors nor will it count to the number of passed sheets required for the exam. I.e. you do not have to complete this sheet but we highly recommend that you solve the assignments as part of your preparations for the exam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46e1c3d4f201b50f67b6cfc07751c839",
     "grade": false,
     "grade_id": "cell-cf105a7b0946d5a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What are the goals of *computer vision* and *image processing*? Name some subtasks. Give one example problem and describe how to solve it with the algorithms presented in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d5e2e5afe7c9b09609a35d08a09a85f",
     "grade": true,
     "grade_id": "cell-981c95da9b28e0cb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Goal of CV**: Recognition of the image by the computer\n",
    "- detection of regions of interest\n",
    "- boundary detection\n",
    "- feature extraction\n",
    "- classification of colors, shapes, objects\n",
    "- 3D representations of real scenes\n",
    "- reconstruction of 3D surfaces\n",
    "- motion detection\n",
    "    - object / background separation\n",
    "    - direction and velocity computation\n",
    "    - object tracking\n",
    "\n",
    "**Goal of image processing**: Enhance images to facilitate analysis by a human\n",
    "- repair corrupted images\n",
    "- compensation of bad acquisition conditions (e.g. contrast enhancement)\n",
    "- improve perceptibility (e.g. contrast enhancement)\n",
    "- 'highlight' information\n",
    "\n",
    "**Example problem: Basic object recognition**\n",
    "\n",
    "A simple approach for object recognition is **template matching**:\n",
    "- construct a template (prototypical model of the object you'd like to find in the image)\n",
    "- search for template in image by computing  similarity between template and underlying image patch\n",
    "    - two similarity measures\n",
    "        - mean absolute difference (MAD)\n",
    "        - correlation coefficient (better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e226c4bb5a23abc24cec50dc8305e07",
     "grade": false,
     "grade_id": "cell-9a9675262ebed05c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Describe the difference between *top down* and *bottom up* strategies. From another perspective they are called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfc6574b51f2620009b4d38ecfbfc413",
     "grade": true,
     "grade_id": "cell-67ae38f906dffb4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Those are processing strategies:\n",
    "\n",
    "**bottom up**:\n",
    "- starting from data\n",
    "- looking for increasingly complex features and connections until they match the model\n",
    "- aka **data driven**\n",
    "\n",
    "**top down**:\n",
    "- try to find model within data\n",
    "- aka **model driven**\n",
    "\n",
    "Commonly a mixture of both is used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "732db4eb328e74903cc7d3fc05b082ce",
     "grade": false,
     "grade_id": "cell-e983b33b6ab2c9bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** What is the semantic gap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a4dcb56b861c296a15dee55ab0e69e0",
     "grade": true,
     "grade_id": "cell-a838a5e5b650ec7f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The **semantic gap** refers to the hope for a correlation between low level features and high level concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88732df1e711bb794c2875e583c5dec2",
     "grade": false,
     "grade_id": "cell-37d0783e273087db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Image Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "657498aa8ea4757ca05952f5826987c5",
     "grade": false,
     "grade_id": "cell-b7355fd2edf5f709",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Draw (on paper) the concept of a pinhole camera. Draw at least an object, rays, the pinhole, and the image plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c8a254c2f40b91f3bc8bce3b8351c73",
     "grade": true,
     "grade_id": "cell-cd296b0a201d73ec",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "<img src=\"img/pinhole.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "476f6bf46289e53e66d82147471036cc",
     "grade": false,
     "grade_id": "cell-1703b55fbd639694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Explain how human color vision works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a70984db54a02d314173ce58dd16c68",
     "grade": true,
     "grade_id": "cell-8de55ef4fc3478a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- visible wavelengths: $\\approx 380 nm - 750 nm$\n",
    "- the eye contains three types of receptors with different spectral sensitivities (RGB)\n",
    "- arranged side by side in the retina\n",
    "- so we reduce the incoming spectrum to just three stimuli $\\infty \\rightarrow 3$ dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff1dade0d09d369cff2a141a664fe524",
     "grade": false,
     "grade_id": "cell-fccfd92b5636da36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Is a Bayer-Filter a local operator? Explain your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f17d5367fb0260dda501438827f53d2",
     "grade": true,
     "grade_id": "cell-ce1bb2a95756bb3c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "No, even though it's also called filter (just like filter kernels), it's a pixel layout of a 1-chip camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65577ae1b51e05e4b4cd48089c829c17",
     "grade": false,
     "grade_id": "cell-126352b10a73aa23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**d)**  What is the smallest distance between two pixels under 4-/8-neighborhood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31525618c7fb2075060cd48415b0e74d",
     "grade": true,
     "grade_id": "cell-6f0e5d8f9e9c507e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$4$-neighborhood: **manhattan distance**: $|x_1 - x_2| + |y_1 - y_2|$  \n",
    "$\\rightarrow$ In $4$-neighborhood, you can move only up/down/left/right, that's manhattan distance\n",
    "\n",
    "$8$-neighborhood: **chessboard distance**: $\\max (|x_1 - x_2|, |y_1 - y_2|)$  \n",
    "$\\rightarrow$ In $8$-neighborhood, you can move as the king in chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f90f21010ac694a73a5b4606d2668dbe",
     "grade": false,
     "grade_id": "cell-c9096dca32ed1cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**e)** Name the two types of loss of information and give an example for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c2a95295433ec7b0d2fdde5104656cb",
     "grade": true,
     "grade_id": "cell-9d87ecab01da1ef8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Stochastic loss:** E.g. noise\n",
    "\n",
    "**Deterministic loss:** E.g. projection and sampling, bad camera parameters (over-/underexposure, bad focus), motion blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1479f0ec237ce559e224ff1743afc853",
     "grade": false,
     "grade_id": "cell-6d84b6e4de4fd660",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Basic Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21a3b2e78227337dbf04d79a13fb6351",
     "grade": false,
     "grade_id": "cell-34f94d134a296642",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *point operator*, a *local operator*, a *global operator*? Provide examples for each of them. Which are *linear* which are not? Give an example for a *non-homogenous* operator. Describe application scenarios for different operators. What is a *rank filter*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5eb3769833eead0396b6d99620dea584",
     "grade": true,
     "grade_id": "cell-3d2c26876b96142e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**point operator:** $g'(x, y) = O(g(x, y))$ $\\rightarrow$ result pixel depends only on input pixel\n",
    "- e.g. **thresholding**: $g'(x, y) = \\Theta (g(x, y) - \\vartheta)$ with threshold $\\vartheta$ and $\\Theta (x) = 0$ for $x < 0$ and $\\Theta (x) = 1$ otherwise\n",
    "    - non-linear, it does matter whether you first multiply each pixel by scalar and then apply or vice versa\n",
    "    - application example: binarization of a bimodal distribution\n",
    "- another example would be a **linear transform**, e.g. $g'(x, y) = a \\cdot g(x, y) + b$\n",
    "    - obviously linear\n",
    "    - application example: luminance- and contrast-enhancement\n",
    "\n",
    "**local operator:** $g'(x, y) = O(g(x, y), g(surroundings(x, y)))$ $\\rightarrow$ result pixel depends on input pixel + surrounding pixels\n",
    "- e.g. **convolution** defined by filter kernel: $g'(x, y) = \\sum_{i \\in [-m, m]} \\sum_{j \\in [-n, n]} k(i+m, j+n) \\cdot g(x+i, y+j)$ (scalar product of kernel and image patch)\n",
    "    - linear (scalar product, we had to prove that)\n",
    "    - application example: smoothing, edge detection\n",
    "\n",
    "**global operator:** $g'(x, y) = O(g(all pixels))$ $\\rightarrow$ result pixel depends on all pixels of the input image\n",
    "- e.g. **Fourier transform**: Transforms image $g$ from the spatial domain to the frequency domain\n",
    "    - linear (additivity and homogeneity hold)\n",
    "    - application example: \n",
    "        - fast computation of convolution in Fourier space (just a multiplication)\n",
    "        - detect texture in images\n",
    "        - compression\n",
    "\n",
    "**non-homogeneous operator**\n",
    " - depends explicitly on the location: $g'(x, y) = O(g(x, y), x, y)$\n",
    " - means that the operator does change based on the pixel location (different behavior)\n",
    " - e.g. darken upper part of the image such as a bright sky (depends on $y$ value)\n",
    "\n",
    "**Rank filter:**\n",
    "- local operator\n",
    "- non-linear (can not be implemented as convolution)\n",
    "- sort gray values covered by kernel\n",
    "- select gray value from sorted list that replaces the current pixel (result)\n",
    "- the selection of the position determines the type of rank filter:\n",
    "    - **min filter**: select min gray value (first position)\n",
    "    - **median filter**: select center of the list\n",
    "    - **max filter**: select max gray value (last position)\n",
    "- rank filters can be used for several purposes:\n",
    "    - image quality enhancement, e.g. smoothing, sharpening\n",
    "    - image pre-processing, e.g. noise reduction, contrast enhancement\n",
    "    - feature extraction, e.g. border detection, isolated point detection\n",
    "    - image post-processing, e.g. small object removal, object grouping, contour smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15456799b1386d372b08f0ba12b6dc04",
     "grade": false,
     "grade_id": "cell-0c6d98ee6c0ededf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Load an image and apply different local operators (convolution, nonlinear smoothing, morphological) and display the results. Explain their effects and possible applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3451bcd3d69d20fb7c0c4f7244f2be12",
     "grade": true,
     "grade_id": "cell-e6bb5f892904ac23",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage import filters, morphology\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_test_img():\n",
    "    return plt.imread('img/test.JPG')\n",
    "\n",
    "def get_test_img_gray():\n",
    "    img = get_test_img()\n",
    "    black_white = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for x in range(img.shape[0]):\n",
    "        for y in range(img.shape[1]):\n",
    "            black_white[x][y] = np.sum(img[x][y]) / 3\n",
    "    return black_white / black_white.max()\n",
    "\n",
    "def generate_noisy_img(img):\n",
    "    prob = 0.5\n",
    "    noisy_img = img.copy()\n",
    "\n",
    "    for x in range(1, noisy_img.shape[0] - 1, 2):\n",
    "        for y in range(1, noisy_img.shape[1] - 1, 4):\n",
    "            # add noise\n",
    "            if random.random() < prob:\n",
    "                if random.choice([0, 1]) == 0:\n",
    "                    noisy_img[x][y] = 0\n",
    "                    noisy_img[x - 1][y] = 0\n",
    "                    noisy_img[x + 1][y] = 0\n",
    "                else:\n",
    "                    noisy_img[x][y] = 1\n",
    "                    noisy_img[x - 1][y] = 1\n",
    "                    noisy_img[x + 1][y] = 1\n",
    "    return noisy_img\n",
    "\n",
    "def apply_laplace(img):\n",
    "    # laplace is really prone to noise --> smooth first via gaussian\n",
    "    img = filters.gaussian(img)\n",
    "    return filters.laplace(img)\n",
    "\n",
    "def get_binarized_img(img, thresh):\n",
    "    tmp = img.copy()\n",
    "    for x in range(img.shape[0]):\n",
    "        for y in range(img.shape[1]):\n",
    "            if tmp[x][y] < thresh:\n",
    "                tmp[x][y] = 1\n",
    "            else:\n",
    "                tmp[x][y] = 0\n",
    "    return tmp\n",
    "\n",
    "plt.figure(figsize=(26, 10))\n",
    "\n",
    "img = get_test_img_gray()\n",
    "\n",
    "plt.subplot(2, 7, 1); plt.title('original image'); plt.imshow(img, cmap='gray')\n",
    "plt.subplot(2, 7, 2); plt.title('conv gaussian'); plt.imshow(filters.gaussian(img, sigma=2.5), cmap='gray')\n",
    "# there's also sobel_v and sobel_h for just using the vertical / horizontal version\n",
    "plt.subplot(2, 7, 3); plt.title('conv sobel'); plt.imshow(filters.sobel(img), cmap='gray')\n",
    "plt.subplot(2, 7, 4); plt.title('noisy img'); plt.imshow(generate_noisy_img(img), cmap='gray')\n",
    "plt.subplot(2, 7, 5); plt.title('median filtering of noisy img'); plt.imshow(filters.rank.median(generate_noisy_img(img)), cmap='gray')\n",
    "plt.subplot(2, 7, 6); plt.title('rank max'); plt.imshow(filters.rank.maximum(img, morphology.disk(10)), cmap='gray')\n",
    "plt.subplot(2, 7, 7); plt.title('binarized img'); plt.imshow(get_binarized_img(img, 0.2), cmap='gray')\n",
    "plt.subplot(2, 7, 8); plt.title('erosion of binarized img'); plt.imshow(morphology.binary_erosion(get_binarized_img(img, 0.2)), cmap='gray')\n",
    "plt.subplot(2, 7, 9); plt.title('dilation of binarized img'); plt.imshow(morphology.binary_dilation(get_binarized_img(img, 0.2)), cmap='gray')\n",
    "plt.subplot(2, 7, 10); plt.title('binary opening'); plt.imshow(morphology.binary_opening(get_binarized_img(img, 0.2)), cmap='gray')\n",
    "plt.subplot(2, 7, 11); plt.title('binary closing'); plt.imshow(morphology.binary_closing(get_binarized_img(img, 0.2)), cmap='gray')\n",
    "plt.subplot(2, 7, 12); plt.title('non-linear smoothing (rank - mean)'); plt.imshow(filters.rank.mean(img, morphology.disk(10)), cmap='gray')\n",
    "plt.subplot(2, 7, 13); plt.title('conv laplace'); plt.imshow(apply_laplace(img), cmap='gray')\n",
    "plt.subplot(2, 7, 14); plt.title('enhanced contrast'); plt.imshow(filters.rank.enhance_contrast(img, morphology.disk(10)), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcbdd31faa85d6cb8a30301a30ba0f55",
     "grade": true,
     "grade_id": "cell-af283fbda9c16aba",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Effects and possible applications**\n",
    "- Convolution with **Gaussian** filter\n",
    "    - smoothing (blurring)\n",
    "    - noise reduction\n",
    "- Convolution with **Sobel** filter\n",
    "    - edge detection (horizontal, vertical, diagonal)\n",
    "    - corresponds to first derivative of pixel intensity values\n",
    "    - e.g. for object detection\n",
    "- Rank filter **median**\n",
    "    - sort gray values covered by kernel\n",
    "    - select center\n",
    "    - e.g. removing noise (very good)\n",
    "- Rank filter **max**\n",
    "    - sort gray values covered by kernel\n",
    "    - select max\n",
    "    - e.g. brightening?\n",
    "- **Binarized** image\n",
    "    - thresholded (below thresh $0$, above $1$)\n",
    "- **Erosion** of binarized image\n",
    "    - removes irregularities\n",
    "    - cuts off fringe of objects\n",
    "- **Dilation** of binarized image\n",
    "    - enlarges objects (adds pixel at fringe)\n",
    "    - fills up holes\n",
    "- **Opening**\n",
    "    - compound operation: erosion followed by dilation\n",
    "    - e.g. remove irregularities without making object smaller\n",
    "- **Closing**\n",
    "    - compound operation: dilation followed by erosion\n",
    "    - e.g. fill up holes without making object larger\n",
    "- **Non-linear smoothing** by rank mean\n",
    "    - rank filter that takes mean of image patch\n",
    "    - isn't that the same as a box filter?\n",
    "    - e.g. median filter - remove outliers\n",
    "- Convolution with **Laplace** filter\n",
    "    - edge detection (independent of direction)\n",
    "    - detects gray value jumps\n",
    "    - corresponds to second derivative of pixel intensity values\n",
    "    - very noise-sensitive; should be smoothed before application\n",
    "- **Contrast enhancement** by rank filter\n",
    "    - replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum\n",
    "    - otherwise it is replaced by the local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64650978b2d4ab9174fe62436d4db5dc",
     "grade": false,
     "grade_id": "cell-664ef6f4776a3841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)**\n",
    "With pen and paper: Generate a random  $5 \\times 5$ image and smooth this image by a $3 \\times 3$ laplace filter. Select a border handling mode of your choice."
   ]
  },
  {
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = np.array([[0, 100, 200, 100, 0], [0, 0, 200, 0, 0], [0, 0, 200, 0, 0], [0, 0, 100, 0, 0], [0, 0, 100, 0, 0]])\n",
    "laplace_kernel = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]])\n",
    "laplace = ndimage.convolve(img, laplace_kernel, mode='nearest')\n",
    "\n",
    "print(\"result:\\n\", laplace)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1); plt.title('original img'); plt.imshow(img, cmap='gray')\n",
    "plt.subplot(1, 2, 2); plt.title('laplace filtered'); plt.imshow(laplace, cmap='gray')\n",
    "\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "**d)** Give an example $3\\times3$ kernel for the following filters and briefly explain their use:\n",
    "* Box\n",
    "* Binomial\n",
    "* Sobel (one direction of your choice)\n",
    "* Laplace"
   ],
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a2278f2eef1cbeecb3bab990484b14c",
     "grade": false,
     "grade_id": "cell-8245a3a223ddeb29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "785356241ecc60f56723f0a4c54fa70b",
     "grade": true,
     "grade_id": "cell-b6b155547f7deec2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Box Filter**\n",
    "\n",
    "- smoothing, e.g. for noise reduction\n",
    "- super simple, just averaging\n",
    "- problem: hard border (one pixel completely in, next completely out)\n",
    "\n",
    "$k_{Box} = \\frac{1}{9} \\cdot\n",
    "\\left[ \\begin{array}{rrr}\n",
    "1 & 1 & 1 \\\\ \n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\ \n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Binomial Filter**\n",
    "- discrete approximation of Gaussian kernel\n",
    "- makes hard border of box filter smooth\n",
    "- also smoothing (noise reduction), but better\n",
    "\n",
    "$k_{Bin} = \\frac{1}{16} \\cdot\n",
    "\\left[ \\begin{matrix}\n",
    "1 & 2 & 1 \\\\ \n",
    "2 & 4 & 2 \\\\ \n",
    "1 & 2 & 1 \\\\ \n",
    "\\end{matrix} \\right]$\n",
    "\n",
    "**Sobel Filter (vertical version)**\n",
    "\n",
    "- edge detection (for vertical edges)\n",
    "- corresponds to 1st derivative of pixel intensities\n",
    "\n",
    "$k_{Sobel} = \\frac{1}{4} \\cdot\n",
    "\\left[ \\begin{matrix}\n",
    "1 & 0 & -1 \\\\ \n",
    "2 & 0 & -2 \\\\ \n",
    "1 & 0 & -1 \\\\ \n",
    "\\end{matrix} \\right]$\n",
    "\n",
    "**Laplace Filter**\n",
    "\n",
    "- detects jumps of gray values (edges)\n",
    "- edges in all direction\n",
    "- corresponds to 2nd derivative of pixel intensities\n",
    "\n",
    "$k_{Laplace} = \n",
    "\\left[ \\begin{matrix}\n",
    "0 & 1 & 0 \\\\ \n",
    "1 & -4 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "\\end{matrix} \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbf6cf2fe3566a959a9e2899ec8173bb",
     "grade": false,
     "grade_id": "cell-5fc6bfdddcafb687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**e)** What are separable filter kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6d897d6fc5fb2f9e9dc459c51a93e69",
     "grade": true,
     "grade_id": "cell-f560b5c81610d0d7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The computational effort for an $m \\times n$ kernel is $O(mn)$.  \n",
    "Some kernels are separable, i.e. are product of row vector and column vector, which leads to a more efficient convolution $O(m+n)$.  \n",
    "\n",
    "Examples for separable kernels are Gaussian, Sobel, and Box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4eb3ca7eedc89cf777ee5a0d9923c3d",
     "grade": false,
     "grade_id": "cell-0950af9dd4a0ca1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Image Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30e6d7c8ab1bcd51a63f5944c26181dc",
     "grade": false,
     "grade_id": "cell-7b4bae6d11f700f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)**  What is the histogram of an image? What is a gradient image and how is it computed? What is a histogram of gradients? Name some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dafc09c368a24f8e4a0bd4ae5dc927e",
     "grade": true,
     "grade_id": "cell-aa77731fc3578928",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "A **histogram** of a grayscale image counts the occurrences for each gray value in the image.  \n",
    "Application examples:\n",
    "- find threshold for binarization of a bimodal distribution\n",
    "- histogram equalization to enhance contrast\n",
    "\n",
    "An image **gradient** is a directional change in the intensity or color in an image:  \n",
    "- gradient of the image is one of the fundamental building blocks in image processing\n",
    "- gradients ($x$ and $y$ derivatives) of an image are useful because the magnitude of gradients is large around edges and corners  \n",
    "(regions of abrupt intensity changes) and edges and corners provide a lot more information about object shape than flat regions\n",
    "- e.g. canny edge detector uses image gradient for edge detection\n",
    "- mathematically, the gradient of a two-variable function (here the image intensity function) at each image point is a 2D  \n",
    " vector with the components given by the derivatives in the horizontal and vertical directions\n",
    "- at each image point, the gradient vector points in the direction of largest possible intensity increase\n",
    "- the length of the gradient vector corresponds to the rate of change in that direction\n",
    "- most common **way to approximate the image gradient is to convolve an image with a kernel**, such as the sobel operator\n",
    "\n",
    "**Gradient image**\n",
    "\n",
    "- image after filtering with a gradient filter, e.g. Sobel\n",
    "- filtering the image with the horizontal and vertical Sobel filters $[-1, 0, 1]$ and $[-1, 0, 1]^T$\n",
    "\n",
    "**Histogram of oriented gradients (HOG)**\n",
    "\n",
    "- to calculate a HOG descriptor, we need to first calculate the horizontal and vertical gradients (sobel)\n",
    "- afterwards, we can get the magnitude and direction of the gradient at every pixel:\n",
    "    - **gradient magnitude**: $m'(x, y) = \\sqrt{\\Delta_x g^2 + \\Delta_y g^2}$\n",
    "    - **direction:** Use the inverse tangent: $\\beta(x, y) = arctan(\\frac{\\Delta_y g}{\\Delta_x g})$\n",
    "- next step is to create a histogram of gradients:\n",
    "    - a bin is selected based on the direction, and the vote (the value that goes into the bin) is selected based on the magnitude\n",
    "- such a HOG can be further processed and become a feature vector that can for example be used in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f38e6ecaec9ae193d20e5c2a71d59ddb",
     "grade": false,
     "grade_id": "cell-08a4b0b6834a4f08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Give formulae for information content and average information content. What do information content and entropy measure? On the slides $\\log_n$ is used for information content and $\\log_2$ is used for entropy. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17712dc185828789b4a46e58fc60bdd4",
     "grade": true,
     "grade_id": "cell-5663863917e695cb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Information content** $I(e) = -\\log_n P(e)$ for $n$-valued symbols where $P(e)$ is the probability of event $e$\n",
    "- measures 'information' based on the idea that less probable events are more informative\n",
    "- quantifying the level of \"surprise\" of a particular outcome\n",
    "- aka surprisal or Shannon information\n",
    "- the information content can be expressed in various units of information, of which the most common is the bit\n",
    "- different choices of base correspond to different units of information\n",
    "\n",
    "**Avg information content aka entropy** $E = - \\sum_{i = 1}^{n} P(e_i) \\log_2 P(e_i)$\n",
    "- measures the average information content and thus\n",
    "    - is very high if every event is equally likely\n",
    "    - very low if there is only one outcome\n",
    "- quantifying how surprising the random variable is \"on average\"\n",
    "- measure on information in terms of uncertainty (chaos)\n",
    "- e.g. if you have a coin that has only one result (heads on both sides), then you know beforehand what the result  \n",
    "  will be and there is no information gain by actually flipping it (entropy $0$)\n",
    "- the entropy of a regular coin toss is $1$ - you can find out whether it's heads or tails with just one question: \"Is it heads?\"\n",
    "- amount of information in $5$ coin flips is $5$ bits\n",
    "- entropy - least amount of questions we have to ask\n",
    "- we have a '$-$' in front of the sum, because we have the $\\log$ of a value between $0$ and $1$ which gives us a negative value and we want our entrpy to be positive\n",
    "    - large uncertainty (chaos) -> high positive number\n",
    "\n",
    "The information content formula with base $n$ is just more general, but would be base $2$ as well for our purposes, we work with bits.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Claude Shannon's definition of information content was chosen to meet several axioms:\n",
    "- an event with probability 100% is perfectly unsurprising and yields no information\n",
    "- the less probable an event is, the more surprising it is and the more information it yields\n",
    "- if two independent events are measured separately, the total amount of information is the sum of the self-informations of the individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "img = np.array([\n",
    "    [0, 1, 3, 7, 1, 0],\n",
    "    [5, 3, 1, 4, 6, 0],\n",
    "    [5, 7, 2, 0, 1, 4],\n",
    "    [4, 2, 2, 5, 3, 7],\n",
    "    [3, 4, 5, 0, 2, 1],\n",
    "    [3, 7, 4, 1, 0, 2]\n",
    "])\n",
    "\n",
    "heights, bins = np.histogram(img, 8)\n",
    "\n",
    "# max entropy for 3 bits would be 3\n",
    "print(\"ENTROPY:\", measure.shannon_entropy(img))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1); plt.title('img'); plt.imshow(img)\n",
    "plt.subplot(1, 2, 2); plt.title('histogram'); plt.bar(bins[:-1], heights)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45280fc27796309416338818dc4d6a6a",
     "grade": false,
     "grade_id": "cell-47c2a4e75b927e20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Discuss histogram equalization. Name some problems and explain how they can be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5361bc1d68bf085ae14d10941b612e6c",
     "grade": true,
     "grade_id": "cell-91dc16c7b7371d5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Histogram equalization** is basically maximizing the information content $I(e) = -log_n P(e)$.  \n",
    "It's based on the idea that an optimal image has maximum information content. Therefore, we maximize the entropy $E$ of the image's histogram.  \n",
    "The entropy is just the average information content and is defined as $E = - \\sum_{i=1...N} P(e_i) \\cdot log_2 P(e_i)$.\n",
    "\n",
    "The **entropy of the image is maximized** if $P(g) = const$ for all $g = 0, ..., 255$.\n",
    "\n",
    "To maximize the entropy, we need to apply a transfer function. For continuous histograms with unit norm, it's just $g'(g) = \\int_{0...g} H(w) dw$. \n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  \n",
    "- **High level idea**\n",
    "    - HE is used to improve contrast in images\n",
    "    - it accomplishes this by effectively spreading out the most frequent intensity values\n",
    "        - i.e. stretching out the intensity range of the image\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  \n",
    "\n",
    "Problem: $g'(g)$ transforms $g$ to real values which need to be rounded (loss of information).\n",
    "\n",
    "Another problem is that the transformation of gray values is not related to image content. Thus, unimportant aspects may  \n",
    "be enhanced while important aspects vanish.\n",
    "\n",
    "How to address the **problems**? -> Several refinements:\n",
    "- **Adaptive histogram equalization (AHE)**\n",
    "    - histogram for each pixel individually\n",
    "    - individual transfer function for each pixel\n",
    "- **Contrast limited AHE (CLAHE)**\n",
    "    - like AHE, but contrast is enhanced only up to predefined limit\n",
    "    - prevents excessive local contrast enhancements (e.g. for background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e01a786a94d1f225d7e52102e3eaceb",
     "grade": false,
     "grade_id": "cell-68f5fc93fc33cf4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Morphological operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a272d42d880400db55982bcf141f072",
     "grade": false,
     "grade_id": "cell-4f367bf552281c08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** What is a structuring element? How is it applied in erosion and dilation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10072f1c6bf76a62b34641a95d6676fc",
     "grade": true,
     "grade_id": "cell-da5d3fe984177324",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The structuring element is a binary matrix that specifies a morphological operation.  \n",
    "It is moved across the image line by line like a filter kernel.  \n",
    "The structuring element is compared to the underlying image patch and the resulting similarity value is assigned  \n",
    "to a result matrix at the location corresponding to the anchor point of the structuring element (usually the center).\n",
    "\n",
    "**Erosion**\n",
    "- assign $1$ to result pixel if all $1$-elements of the structuring element cover $1$-pixels of the image, else assign $0$\n",
    "    - conjunction of implications where a $1$ in the structuring element implies a $1$ in the underlying image patch\n",
    "    - cuts off fringe of objects\n",
    "\n",
    "**Dilation**\n",
    "- assign $1$ to result pixel if at least one $1$-element of the structuring element covers a $1$-pixel in the image, else assign $0$\n",
    "    - disjunction of conjunctions of pair pixels between structuring element and underlying image patch\n",
    "    - adds pixels at the fringe ob objects and fills holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import morphology\n",
    "\n",
    "img = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "struct_one = np.array([[1, 1]])\n",
    "struct_two = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 5, 1); plt.title('img'); plt.imshow(img)\n",
    "plt.subplot(1, 5, 2); plt.title('erosion S1'); plt.imshow(morphology.binary_erosion(img, struct_one))\n",
    "plt.subplot(1, 5, 3); plt.title('dilation S1'); plt.imshow(morphology.binary_dilation(img, struct_one))\n",
    "plt.subplot(1, 5, 4); plt.title('erosion S2'); plt.imshow(morphology.binary_erosion(img, struct_two))\n",
    "plt.subplot(1, 5, 5); plt.title('dilation S2'); plt.imshow(morphology.binary_dilation(img, struct_two))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "# hit-or-miss\n",
    "\n",
    "img = np.array([\n",
    "    [0, 0, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "struct_elem = np.array([\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 3, 1); plt.title('img'); plt.imshow(img)\n",
    "plt.subplot(1, 3, 2); plt.title('struct elem'); plt.imshow(struct_elem)\n",
    "plt.subplot(1, 3, 3); plt.title('hit-or-miss'); plt.imshow(ndimage.binary_hit_or_miss(img, struct_elem))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "900a792d3a253cdbfe693b460fd95e60",
     "grade": false,
     "grade_id": "cell-fc1713bbf0ef3b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** Give pseudocode for the distance transform using morphological operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adbdab9e94003294739d6ba16aed2343",
     "grade": true,
     "grade_id": "cell-d929678b78d7912d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "given: image $g$, structuring element $S$\n",
    "\n",
    "$g^0_{boundary} = g \\backslash (g \\ominus S)$ yields the boundary (set of all pixels with distance $0$ to the boundary)\n",
    "\n",
    "$g \\ominus S$ is the object without boundary\n",
    "\n",
    "$g^n_{boundary} = (g(\\ominus S)^n) \\backslash (g(\\ominus S)^{(n+1)})$ where $(\\ominus S)^n$ is short for the $n$-time erosion using $S$\n",
    "\n",
    "The distance transform $D$ is obtained from the union of the boundaries of all distances:\n",
    "\n",
    "$D = \\bigcup_{n=1}^{\\infty} n \\cdot g^n_{boundary}$\n",
    "\n",
    "$\\rightarrow$ not Euclidean distance, but manhattan or chessboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "img = np.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "def my_boundary(img):\n",
    "    erosion = morphology.binary_erosion(img)\n",
    "    return np.logical_xor(erosion, img)\n",
    "\n",
    "def my_distance_transform(img):    \n",
    "    dt = np.zeros(img.shape, np.int32)\n",
    "    lvl = 1\n",
    "    # erode until nothing is left\n",
    "    while np.any(img):\n",
    "        boundary = my_boundary(img)\n",
    "        # pixels with manhattan distance lvl to the boundary\n",
    "        dt[boundary] = lvl\n",
    "        # object without boundary\n",
    "        img = morphology.binary_erosion(img)\n",
    "        lvl += 1\n",
    "    return dt\n",
    "\n",
    "dt = morphology.distance_transform_cdt(img, metric='manhattan')\n",
    "dt_own = my_distance_transform(img)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1); plt.title('img'); plt.imshow(img)\n",
    "plt.subplot(1, 3, 2); plt.title('distance transform'); plt.imshow(dt)\n",
    "plt.subplot(1, 3, 3); plt.title('own implementation'); plt.imshow(dt_own)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08b03941202812effaeaa996b50e41d4",
     "grade": false,
     "grade_id": "cell-bb3d97b79ce3c32c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2841c6e4f4fad9ee515203a0f9684e68",
     "grade": false,
     "grade_id": "cell-3761c09a73f38e51",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Which of the follwoing use additive color mixing and which use subtractive color mixing:\n",
    "* Printer -> **subtractive**\n",
    "* Cathode ray tube (old screens) -> **additive**\n",
    "* LCD screen -> **subtractive**\n",
    "* Van Gogh -> **subtractive**\n",
    "* Analog cinema projector -> **subtractive**\n",
    "* Digital projector (DLP) -> **additive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e3c25113e146776c34bd45c35ecd912",
     "grade": false,
     "grade_id": "cell-437bfb22b0953472",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Name two color spaces and list their advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8820f89d8e53d435ae6825b23eb2830f",
     "grade": true,
     "grade_id": "cell-ee5a11a5f9a6e97c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**RGB (Red Green Blue)**\n",
    "- primary colors based on CIE experiments\n",
    "- well-suited for computer screens, because the pixels are self-luminous -> additive color mixing\n",
    "\n",
    "**CMYK (Cyan (absorbs R) Magenta (absorbs G) Yellow (absorbs B) Key (black - absorbs all))**\n",
    "- color printing is based on subtractive mixing of CYMK\n",
    "- for not self-luminous materials\n",
    "- CMY is complementary to RGB\n",
    "- in principle, K (black) is not necessary, because it can be mixed from CMY\n",
    "- in practice, CMY can only mix a dark blue, so black is used in printing as an additional color\n",
    "\n",
    "**HSV**\n",
    "- based on perception and verbal description of colors\n",
    "- colors can be mixed and described using HSV more easily than using primary colors\n",
    "- **Hue**: angle on color circle (0째 red, 120째 green, 240째 blue)\n",
    "- **Saturation**: 0% no color, 100% pure color\n",
    "- **Value**: Percentage of maximum brightness\n",
    "\n",
    "**Lab**\n",
    "- perceived color distance corresponds to Euclidean distance in Lab space\n",
    "- a: gree-red, b: blue-yellow, L: luminance\n",
    "\n",
    "**YCbCr**\n",
    "- separate luminance and color\n",
    "- luminance -> brightness of grayscale img\n",
    "- $Cb$ -> blueness of the img, $Cr$ -> redness of the img\n",
    "- the human eye is a lot more sensitive to luminance (changes in intensity) than to color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec888b9f995d3b6dd96e4f4eb981df13",
     "grade": false,
     "grade_id": "cell-c06d6b2ac8b2332a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d4901a73217149bc47e7d81c5a1ef2",
     "grade": false,
     "grade_id": "cell-74bc7e2ff33d48ee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain *region based* and *edged based* *segmentation*. What are the differences between *split and merge* and *region merging*? What is the idea of *color segmentation* and does it give any advantage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caeac0e23beaa3cf5373194bd6bd534d",
     "grade": true,
     "grade_id": "cell-b505cd1e5ca22b5e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**In short:**\n",
    "- **region based**\n",
    "    - find regions based on homogeneity measure\n",
    "- **edge based**\n",
    "    - find borders of regions and hope that they enclose meaningful segments\n",
    "- **split-and-merge**\n",
    "    - implicit multiscale strategy, better runtime\n",
    "- **region merging**\n",
    "    - explicit multiscale strategy, high runtime\n",
    "- **color**\n",
    "    - finding color clusters in color space\n",
    "    - can exploit color information\n",
    "    - but usefulness depends on domain\n",
    "\n",
    "**Region based segmentation**\n",
    "- each segment must be homogeneous\n",
    "- what we want: high level concepts such as water, building, sky\n",
    "- what we have: low level features such as color and texture\n",
    "- we hope for correlation of low level features with high level concepts\n",
    "- multiple scales need to be considered to avoid chaining (locally homogeneous, but globally inhomogeneous)\n",
    "- ensures homogeneity within regions - we hope for sharp region boundaries\n",
    "- what we have:\n",
    "    - multi-scale strategies\n",
    "    - region merging, split-and-merge\n",
    "    - texture segmentation\n",
    "\n",
    "**Edge based segmentation**\n",
    "- segments are found by search for boundaries between regions of different features\n",
    "- we hope for homogeneity within regions\n",
    "- advantage: more robust against shading\n",
    "- what we do:\n",
    "    - compute gradient\n",
    "    - edge point detection, e.g. by thresholding gradient magnitude\n",
    "    - region labeling based on edge points\n",
    "    - problem: edge points are not yet connected edges\n",
    "- four methods:\n",
    "    - edge linking\n",
    "    - canny operator\n",
    "    - detection of zero crossings\n",
    "    - watershed transform\n",
    "\n",
    "**Region Merging**\n",
    "- image is mapped to RAG (region adjacency graph)\n",
    "    - segment -> node\n",
    "    - neighborhood of two segments -> edge\n",
    "- init: each pixel is segment (node)\n",
    "- repeat\n",
    "    - search for edges that satisfy homogeneity condition\n",
    "    - merge nodes where edge has best homogeneity\n",
    "- until\n",
    "    - no edge fulfills homogeneity condition\n",
    "\n",
    "**Split-and-Merge**\n",
    "- init: entire image is single segment\n",
    "- splitting: divide segment interatively into four sub-segments as long as homogeneity condition not fulfilled\n",
    "- merging: merge neighboring segments if they fulfill the homogeneity condition even after merging\n",
    "- result: complete and unique decomposition\n",
    "\n",
    "**Split-and-Merge vs. Region Merging**\n",
    "- split-and-merge has significantly less computational effort, because splitting usually stops before pixel level is reached\n",
    "- split-and-merge has also the advantage that large segments are better suited for estimation of distribution than just a few pixels\n",
    "\n",
    "**Color segmentation**\n",
    "- idea: color contains much more information than gray values\n",
    "- aim: find segments of constant color\n",
    "- homogeneity condition: distance / similarity measure in a color space\n",
    "- numerous algos (k-Means clustering, mean shift segmentation)\n",
    "- advantage: color classes of image can be found using clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4750e4eda83d257aee3a5ffb45f22b87",
     "grade": false,
     "grade_id": "cell-d3af45d1f9eef2e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)**  Provide pseudocode for the $k$-means clustering algorithm in color space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d71fdac1cc911ba50e7beafdab61b69c",
     "grade": true,
     "grade_id": "cell-1be957ca47d5e89d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- init $k$ vectors as cluster centers with random RGB values (in RGB space)\n",
    "- $k$ is the number of different colors to find in the data set\n",
    "- assign each pixel to its closest cluster center\n",
    "- recompute cluster center for each cluster\n",
    "- until conversion (only negligible changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09b13f36516a04ca444293afb093446b",
     "grade": false,
     "grade_id": "cell-b72144ad66f6a645",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Give two examples for interactive segmentation and discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6664bb730ea023f244b2fbb2e8f33ca4",
     "grade": true,
     "grade_id": "cell-904df331122ecae0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Interactive region growing**\n",
    "- **flood fill** (fill a homogeneous region with a label) for a gray value image using a single seed point\n",
    "    - problems:\n",
    "        - leakage of region\n",
    "        - region may be very small\n",
    "        - sensitive to noise\n",
    "        - shading\n",
    "\n",
    "**Interactive edge search** (edge following)\n",
    "- user sets starting point\n",
    "- search perpendicular to current gradient direction\n",
    "- edge point found if\n",
    "    - sufficiently close\n",
    "    - exhibits sufficient gradient\n",
    "    - direction similar to current direction\n",
    "- problems:\n",
    "    - noise\n",
    "    - varying gradient strength\n",
    "    - sudden changes of direction of the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d99bdf88833f1d137d4fa996d3f25b63",
     "grade": false,
     "grade_id": "cell-c06d6b2ac8b2332g",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Hough Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a477013829b015d929e5568e96c952b",
     "grade": false,
     "grade_id": "cell-858a0894cc7e02c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is the idea of *Hough transform*? What is an *accumulator space*? How to determine its dimensionality? Can you interpret the linear Hough space? How many dimensions has the accumulator space for circular Hough transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dde0882674d75216ad357c95fb41d107",
     "grade": true,
     "grade_id": "cell-016a8ede5313ac89",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The original **Hough transform** is a method to detect points on straight lines, but it can be generalized to arbitrary geometrical shapes.  \n",
    "\n",
    "There is a kind of voting procedure that is carried out in a parameter space, from which shape candidates are obtained as local maxima.  \n",
    "This space is called **accumulator space** and it is constructed by spanning the space of possible parameter combinations.  \n",
    "Therefore, the dimensionality is determined by the number of parameters.\n",
    "\n",
    "**Example for straight lines**\n",
    "\n",
    "Lines can be represented as: $r = x \\cos \\theta + y \\sin \\theta$  \n",
    "A point $(x, y)$ in Euclidean space would represent all curves $(r, \\theta)$ in the accumulator space that go through the point.  \n",
    "On the other hand, an $(r, \\theta)$ combination in the accumulator space represents all points $(x, y)$ in Euclidean space that are part of the line.  \n",
    "Thus, points on one line in the Euclidean space intersect in one point ($r, \\theta$) in the accumulator space.\n",
    "\n",
    "The general idea is to transform every data point into the accumulator space (discretized) and count for each parameter combination  \n",
    "how often it occurs (accumulator array). The maximum corresponds to the 'best' correspondence of the shape in the Euclidean space.  \n",
    "In the case of the search for straight lines, one would look for the $(r, \\theta)$ combination with the most hits in the accumulator space.\n",
    "\n",
    "The approach is **model-based** in the sense that you need to have a mathematical model of the shape you are looking for, e.g. a line.\n",
    "\n",
    "The **linear Hough transform** is the procedure described in the above example of using the HT to detect straight lines.  \n",
    "So, as described, the accumulator space is just $2D$ for the two parameters $r$ and $\\theta$.  \n",
    "The element $(r, \\theta)$ with the maximum number of hits in the accumulator array is the best candidate for a line in the original space.  \n",
    "Points on one line in the original space intersect in one point $(r,\\theta)$ in the Hough space.\n",
    "\n",
    "**Meaning of coordinates**:\n",
    "- $r$: perpendicular distance between origin and line\n",
    "- $\\theta$: angle between $x$-axis and the perpendicular line ($r$)\n",
    "\n",
    "Circular HT does for circles what linear HT does for lines. Thus, our model is a circle here.  \n",
    "Points on a circle obey $(x - x_c)^2 + (y - y_c)^2 = r^2$  \n",
    "Therefore, the accumulator space has three dimensions: $x_c, y_c, r$ where $x_c$ and $y_c$ are in the image plane and $r$ the radius of the circle.  \n",
    "\n",
    "**Practical problem:** Since we have a third dimension, we have a way higher computational effort compared to linear HT.  \n",
    "However, we can use a simplification and search only for a given radius $r$.  \n",
    "\n",
    "**Procedure:**\n",
    "- increment counter in accumulator array along a circle of radius $r$ around each edge pixel\n",
    "- accumulation points are centers of circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49479dfafacfe0057b03242b8cd07059",
     "grade": false,
     "grade_id": "cell-recap2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Recap II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1731d10a256b28d63c8ef0ed8582b455",
     "grade": false,
     "grade_id": "cell-recap2a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This sheet is a recap of the second half of the term. Neither do you have to present it to your tutors nor will it count to the number of passed sheets required for the exam. I.e. you do not have to complete this sheet but we highly recomment that you solve the assignments as part of your preparations for the exam. We will discuss the results in the last practice session on February 11. Also, if you have questions on any of the topics, please send them to us and we will discuss them in that session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78c60a7b4f68784e5cc3ba50616e6770",
     "grade": false,
     "grade_id": "cell-fourier",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8e98712c9e4cb3472ba19abdd6759a9",
     "grade": false,
     "grade_id": "cell-fourier-a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is the idea of *Fourier Transform*, and why is it useful for image processing? Can you provide a formula? Why is it called an orthogonal transformation? Which aspects of an image can be recognized in its Fourier transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf7730b4ad7235699d3c1c0fb860f76f",
     "grade": true,
     "grade_id": "cell-fourier-a-solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**In short:**\n",
    "- decomposes image into combination of sine and cosine waves\n",
    "- usefulness\n",
    "    - convolution is just multiplication in Fourier space\n",
    "    - understand and design filters in Fourier space\n",
    "- $\n",
    "F(u, v) = \\sum_{x=0}^{N-1}\\sum_{y=0}^{M-1} f(x,y) \\cdot e^{-i 2 \\pi (\\frac{u}{N}x + \\frac{v}{M}y)}\\\\\n",
    "u = \\frac{k_x N}{2\\pi}\n",
    "$\n",
    "- transformation to an orthogonal basis\n",
    "- contribution of different frequencies\n",
    "\n",
    "+++++++++++++++++++++++++++++++\n",
    "\n",
    "The **Fourier transform** is a mathematical tool that transforms (global operation) the given information (image) into another domain.  \n",
    "In our case, using it for images, we transform the information from the spatial domain into the frequency domain.  \n",
    "Sometimes the frequency space enables more efficient computations, e.g. for convolutions.\n",
    "\n",
    "So, the Fourier transform transforms the signal into the frequency space, where it is a sum (or integral)  \n",
    "of sine waves of different frequencies, each of which represents a frequency component.  \n",
    "A point in that space would be a combination of weighted functions (sin / cos curves).\n",
    "\n",
    "- FT: Representation of an image using a basis of sine and cosine patterns\n",
    "- FT is a global operation\n",
    "- FT transforms image $g$ from the spatial domain to the frequency domain\n",
    "- $g$ is projected onto an orthonormal function system of $2$D sine and cosine functions\n",
    "- FT preserves information and can be inverted as a consequence\n",
    "- applications:\n",
    "    - filtering based on Fourier representation of the image\n",
    "    - filter design based on Fourier representation of the kernel\n",
    "    - pattern recognition: Analyze Fourier transform\n",
    "\n",
    "- 2D FT decomposes image into waves of different frequency and direction\n",
    "- the FT holds the following information for each waves\n",
    "    - **amplitude: brightness** of the waves\n",
    "    - **phase: most of the structural information**\n",
    "\n",
    "- translation in space -> phase shift in Fourier space\n",
    "- rotation in space -> rotation in Fourier space\n",
    "\n",
    "- FFT (fast Fourier transform)\n",
    "    - together with the separability of the FT we get an efficient method for a 2D FT\n",
    "\n",
    "**Convolution Theorem**\n",
    "- convolution in space is a multiplication in Fourier space\n",
    "    - fast computation of convolution in Fourier space\n",
    "- application of conv. theo.:\n",
    "    - transform image\n",
    "    - transform kernels\n",
    "    - multiplication in Fourier space\n",
    "    - transform result back\n",
    "- most effective for large kernels\n",
    "\n",
    "**Border problem**\n",
    "- perdiodic continuation of the image\n",
    "    - automatically done when multiplying image and kernel in frequency space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6ddd73099e844bef15b7c4f60a63a39",
     "grade": false,
     "grade_id": "cell-temp-match",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a02667c463b62388f96b856a270f6efa",
     "grade": false,
     "grade_id": "cell-temp-match-a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the principle of template matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34bd59c71592a01734d316a8911051c6",
     "grade": true,
     "grade_id": "cell-temp-match-a-sol",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The idea is to take a prototypical small image of what you are looking for (template) in the image and move that template  \n",
    "across the image just as in convolution to compare it to the underlying image patch with the goal of finding the part of the image that matches the template.  \n",
    "\n",
    "It's a **model-based** approach - the template is a model of what we are looking for in the image.\n",
    "\n",
    "**Advantages**:\n",
    "- robust against noise\n",
    "- efficient implementation as convolution\n",
    "\n",
    "**Disadvantages**:\n",
    "- little robustness against variation of viewpoint / illumination\n",
    "- gray value scaling can cause problems\n",
    "\n",
    "It's probably good to use it in situations where not much variation of viewpoint and illumination is to be  \n",
    "expected such as a part of quality control in manufacturing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ff9b30b4a9f96512a4e858fa929ecff",
     "grade": false,
     "grade_id": "cell-temp-match-b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** When and why does the correlation coefficient perform better than the mean absolute difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd610443e13243ca8c04ba18c6595c7c",
     "grade": true,
     "grade_id": "cell-temp-match-b-sol",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Correlation coefficient is invariant to a linear transform of gray values, whereas mean average distance takes absolute gray values into account.  \n",
    "Therefore the first performs better when the exposure of template and image are not equal.\n",
    "\n",
    "**MAD**\n",
    "- measure for similarity between template $T(i, j)$ and image $g(x, y)$\n",
    "- idea: mean difference of gray values: $MAD(x, y) = \\frac{1}{mn} \\cdot \\sum_{ij} | g(x+i, y+j) - T(i, j)|$\n",
    "- **advantages:** robust to noise, easy to compute\n",
    "- **disadvantages:** gray value scaling can cause problems, sensitive to rotation\n",
    "\n",
    "**Correlation Coefficient**\n",
    "- computes a correlation coefficient to measure similarity between the image and the template\n",
    "- $C_{g, T} = \\frac{\\sigma_{g, T}}{\\sigma_g \\cdot \\sigma_T}$ where $\\sigma_{g, T}(x, y)$ is the covariance between $g$ and $T(i, j)$\n",
    "  and $\\sigma_g, \\sigma_T$ are the standard deviations of $g$ and $T$\n",
    "- the possible values range from $1$ to $1$, where $-1$ or $+1$ indicate the strongest possible pos / neg correlation and $0$ means that they don't correlate\n",
    "- **advantages:** robust to gray value scaling and noise, rotation invariant\n",
    "- **disadvantages:** not as efficient to compute as MAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed324a53e33992f405f92b9d67aae4a8",
     "grade": false,
     "grade_id": "cell-pattern",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6978c004dbf0b001dfdac888412bf75f",
     "grade": false,
     "grade_id": "cell-pattern-a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What are the principle components of a 2-dimensional data distribution. What are the principle components when of an image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "661c8c98e829bc8bbd1dd77ba2213902",
     "grade": true,
     "grade_id": "cell-pattern-a-sol",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The two principal components of a 2-dimensional data distribution are orthogonal and ordered.  \n",
    "The first one points in the direction of largest variance.\n",
    "\n",
    "With PCA we can find a new basis for our dataset. The basis vectors can be ordered by maximun variance / minimun reconstruction error.  \n",
    "In the best case, each basis vector corresponds to an interpretable feature.\n",
    "\n",
    "Principal components are the eigenvalues of the covariance matrix of a dataset. The eigenvalues give the ordering of the components.\n",
    "\n",
    "Finally, the idea is to not just have an ordering of PCs, but to use them for dimensionality reduction, e.g. take the first $n$ PCs to capture $> 90 \\%$ of the variance.\n",
    "\n",
    "If PCA is applied to images, each image is represented as a 1-dimensional vector with $width \\times height$ entries and the covariance matrix is computed  \n",
    "using these vectors. The eigenvectors are vectors of the same dimension and can be seen as images. In case of face images these eigenvectors are called eigenfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f1b98f4d16e79b41722b222f357c228",
     "grade": false,
     "grade_id": "cell-local",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Local Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6e77e1e6345eca0cbc020c934a6a8f2",
     "grade": false,
     "grade_id": "cell-local-a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Describe the *Moravec* and the *Harris corner detectors*. What are the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "882ffb6dd7489cf925a1271d71b321d6",
     "grade": true,
     "grade_id": "cell-local-a-sol",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Moravec IP operator**\n",
    "\n",
    "The Moravec IP operator **measures the saliency** or \"uniqueness\" of a window arround a pixel.  \n",
    "\n",
    "- idea: a window is salient if it's unique in its surroundings\n",
    "- simplest check for uniqueness: Shift window by one pixel and compare to itself\n",
    "- operator compares whifted window in four directions\n",
    "- detector response at $(x, y)$ is the minimum over the four directions\n",
    "- $E(x, y)$ -> saliency map\n",
    "- corner is detected where $E(x, y)$ exceeds a threshold\n",
    "- problems:\n",
    "    - anisotropic (directon dependent)\n",
    "    - hard window like box filter\n",
    "\n",
    "**Harris corner detector**\n",
    "\n",
    "Harris corner detector uses a Gaussian as isotropic windowing function addressing both shortcommings,  \n",
    "the \"hard\" window and the anisotropy of the Moravec detector.\n",
    "\n",
    "- they use the Gaussian as sliding windowing function\n",
    "- and a structure tensor as an analytical solution to compute the differences\n",
    "    - idea: combine gradients in the neighborhood of a point\n",
    "    - the eigenvalues of the tensor:\n",
    "        - both small: homogeneous area\n",
    "        - one large, one small: edge\n",
    "        - both large: corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41fc26091b4d2e523e79fd76687eff66",
     "grade": false,
     "grade_id": "cell-local-b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What are *local features* and what are they used for? Name some examples? Describe the main steps of SIFT and explain how invariances of the features are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "005d10c08b405826d32d11f9a2f2e9f4",
     "grade": true,
     "grade_id": "cell-local-b-sol",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Local features** are used in object recognition to make it more <ins>**resiliant against object rotation, partial occlusion, illuminence differences, scaling**</ins> etc.  \n",
    "This is achieved by using not the whole object but the most interesting parts of it - local patches that describe the object and are most of the time  \n",
    "available in other representations of that object (for example the wheels and lights of a car, but not it's color).\n",
    "- examples:\n",
    "    - SIFT descriptor\n",
    "    - rectangle feature\n",
    "\n",
    "**Interest points** are points in an image and can be used for object recognition.  \n",
    "They should be salient, i.e. special or rare, either within the image, or with respect to common images.  \n",
    "They should be stable, i.e. should keep positions under disruptions in an image and should remain in the same position  \n",
    "with respect to the physical world in a different image of the same scene (e.g., change of viewpoint or illumination).\n",
    "\n",
    "- idea: object recognition from local patches\n",
    "- method 1: compare to reference image\n",
    "- method 2: search for discriminative features\n",
    "- motivated by human perception:\n",
    "    - scans scene with a few fixations\n",
    "    - fast object identification\n",
    "    - high robustness against variety of appearance\n",
    "- ways to select image patches:\n",
    "    - all (sliding window)\n",
    "    - randomly chosen (not good)\n",
    "    - salient ones (good, but how to determine?)\n",
    "    - aim: the same image patches should be found in transformed versions of an image\n",
    "\n",
    "Various concepts for interest point (IP) detection:\n",
    "- **context free**\n",
    "    - maxima of a saliency measure designed for arbitrary images\n",
    "    - independent of the image in question\n",
    "- **context dependent**\n",
    "    - depends on the particular image (e.g. white spot is salient on black image, but not in general)\n",
    "    - model for IPs\n",
    "\n",
    "Requirements to an IP detector:\n",
    "- saliency: IPs should be \"special\" or \"rare\"\n",
    "- stability: \n",
    "    - IPs should keep positions under disruptions\n",
    "    - IPs should remain in the same position with respect to the physical world in a  \n",
    "     different image of the same scene (e.g. change of viewpoint / illumination)\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  \n",
    "So, in summary, local features are used to recognize objects across several images and different conditions.  \n",
    "For example, such features could be used for a robot in order to localize in a scene.  \n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "**SIFT (Scale Invariant Feature Transform)**\n",
    "\n",
    "- **In short**\n",
    "    - compute scale adaptive interest points\n",
    "    - compute region descriptors from the histogram of gradients\n",
    "    - compare images based on region descriptors\n",
    "    - +++++++++++++++++++\n",
    "\n",
    "- **Procedure**\n",
    "    - detect extrema in scale space to obtain scale invariant feature descriptors:\n",
    "        - construct a Gaussian pyramid (DoG-Pyramid): pyramid of differently Gaussian (increasing sigma) filtered versions of the image\n",
    "        - search for keypoints over all pixels and all scales\n",
    "    - detect stable keypoints:\n",
    "        - look for points with extreme value compared to its local neighbors and neighboring scales\n",
    "        - find positions with subpixel precision\n",
    "    - direction detection to obtain rotation invariant descriptors:\n",
    "        - for each keypoint, we compute a local HOG for its neighborhood\n",
    "        - extraction of one (or more) directions from that histogram\n",
    "        - basically search for the gradient with biggest magnitude and normalize based on that\n",
    "            - normalize all other gradients in the environment such that the direction of the one with the biggest magnitude points upwards\n",
    "            - if you rotate the image, the gradients will be normalized to the same direction (rotation invariant)\n",
    "    - keypoint descriptor:\n",
    "        - obtain an image region based on scale and orientation of the keypoint\n",
    "        - transform to a standardized (normalized) description (SIFT feature) - a $128$-dimensional vector based on local gradients\n",
    "        - use gradient image to achieve stability against change in illumination\n",
    "\n",
    "- **Properties**\n",
    "    - features are invariant against rotation and scaling, but not against translation\n",
    "\n",
    "- **Compare keypoints between images (based on the descriptor)**\n",
    "    - the idea is that you have such a detailed description of the keypoint that the same keypoint in another image would yield a very similar descriptor\n",
    "    - based on a similarity definition, e.g. Euclidean distance between the descriptors (vectors) of two features in two images,  \n",
    "    you can determine whether you found a feature in another image\n",
    "    - to find the same object in two images:\n",
    "        - apply SIFT to both images\n",
    "        - compare descriptor vectors\n",
    "            - if you find a reasonably close pair of vectors, you know that the two points corresponding to the vectors match between the two images\n",
    "            - if you find a sufficiently large number of such corresponding points, you know that you found the object in the other image\n",
    "            - can for example be used for stitching in panorama images\n",
    "\n",
    "- **Why not translation invariant?**\n",
    "    - if we change the perspective (sufficiently strong), it won't work anymore, the gradients are too different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f8ceb0e9be92951bbf59dda58224244",
     "grade": false,
     "grade_id": "cell-recap-compression",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee7124a43a178fbe52dd4eed3bda9675",
     "grade": false,
     "grade_id": "cell-recap-huffmana",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** How does Huffman-Coding work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55361151d9c8c9b43a133ab1be753d91",
     "grade": true,
     "grade_id": "cell-recap-huffmanasolution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Huffman-Coding Steps**\n",
    "- compute normalized histogram\n",
    "- order symbols according to probability\n",
    "- generate tree\n",
    "    - merge the two least likely symbols\n",
    "    - repeat until only two symbols remain\n",
    "- start from the back and generate a prefix-free code according to the probability\n",
    "\n",
    "The result is a coding scheme that assigns the shortest codes to the most likely signals and the longest codes to the least likely ones,\n",
    "thereby removing the coding redundancy.  \n",
    "The theoretical maximum compression factor is given by $\\frac{\\#bits}{entropy}$, which intuitively makes sense, because if the entropy is high ('chaotic' image ~uniformly distributed gray values), there is not much to be compressed (low compression factor). If, on the other hand, the entropy is low (only a few different gray values), there is a lot to be compressed.\n",
    "\n",
    "Huffman coding works particularly well for images with low entropy, that is images where some gray values are dominant.  \n",
    "The entropy of the image is a lower bound for the avg code length when using Huffman coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "import huffman\n",
    "\n",
    "img = np.array([\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 2, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 5, 0],\n",
    "    [4, 4, 1, 2, 3, 0],\n",
    "    [0, 3, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "entropy = measure.shannon_entropy(img)\n",
    "print(\"entropy:\", entropy)\n",
    "print(\"theoretical max compression factor:\", 3 / entropy)\n",
    "\n",
    "occurrences = [(s, np.count_nonzero(img == s)) for s in np.unique(img)]\n",
    "\n",
    "huffman_codes = huffman.codebook(occurrences)\n",
    "print(\"huffman codes\", huffman_codes)\n",
    "print(\"memory consumption before:\", img.size * 3, \"bits\")\n",
    "\n",
    "mem_after = 0\n",
    "for k in huffman_codes:\n",
    "    mem_after += np.count_nonzero(img == k) * len(huffman_codes[k])\n",
    "\n",
    "print(\"memory consumption after:\", mem_after, \"bits\")\n",
    "print(\"actual compression ratio:\", (img.size * 3) / mem_after)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e53f6b4f53e555ca9745ba76fb3aed94",
     "grade": false,
     "grade_id": "cell-recap-huffmanb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b)** What is the Gray code and what is its relation to run length encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24b6dad8652af534e041e2c465c0a4a7",
     "grade": true,
     "grade_id": "cell-recap-huffmanbsolution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Gray code is an ordering of the binary system such that two successive values differ in only one bit (Hamming distance $1$).  \n",
    "\n",
    "The Gray code is useful in the context of run length encoding, if one assumes that neighboring pixels will usually have similar values.  \n",
    "If encoded with a Gray code, this means that one can expect less bit switches between neighboring pixels and hence longer runs  \n",
    "(especially if the different bitmaps of an image are compressed separately).\n",
    "\n",
    "The enhancement results from the fact that successive number differ in only one bit, which means that only one bit plane is disturbed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "763967590defe35c5d24efdedd145b6a",
     "grade": false,
     "grade_id": "cell-224529086cbb36b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Understanding the Wireframe-Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "724b13ac39acd58feae03fe1c54dbd1d",
     "grade": false,
     "grade_id": "cell-14e3efd787c6918e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** Explain in your own words the functions on slide  (CV-12 slide 9). Also explain when and why it may make sense to use $m$ instead of $m'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcc0fd4275905dd07af1acd024a57f0e",
     "grade": true,
     "grade_id": "cell-3b6ee3b2c2a9f798",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "From the initial pose, the wire frame model is iteratively adapted to the image based on gradients.  \n",
    "\n",
    "$m$: magnitude, $\\beta$: orientation, $g(x, y)$: image  \n",
    "\n",
    "**$x$-gradient**: $\\Delta_x g = g(x+1, y) - g(x-1, y) \\rightarrow$ for a fixed $y$, it's the difference between the pixels to the left and to the right  \n",
    "**$y$-gradient**: $\\Delta_y g = g(x, y+1) - g(x, y-1) \\rightarrow$ for a fixed $x$, it's the difference between the pixels to above and below  \n",
    "**gradient magnitude**: $m'(x, y) = \\sqrt{\\Delta_x g^2 + \\Delta_y g^2}$  \n",
    "**orientation:** Use the inverse tangent: $\\beta(x, y) = arctan(\\frac{\\Delta_y g}{\\Delta_x g})$  \n",
    "\n",
    "For the gradient magnitude, there's an alternative computation which is thresholded:  \n",
    "$m(x, y) = \\Theta(m'(x, y) - T)$ (only takes magnitudes that are sufficiently large)  \n",
    "\n",
    "When and why $m$ instead of $m'$?\n",
    "- e.g. when you only want to consider salient edges in the image"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "Rmd,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('cv': conda)",
   "language": "python",
   "name": "python38664bitcvcondace24c6b5e63f40158ccc45b6baeafab5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}