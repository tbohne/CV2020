{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94c949e1247e5585a5306a758b0dfbaa",
     "grade": false,
     "grade_id": "cell-60aa580d9d920dba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "OsnabrÃ¼ck University - Computer Vision (Winter Term 2020/21) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Ludwig Schallner, Artem Petrov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "157ac38f4c7903e7a7ce8f3baf8aec2f",
     "grade": false,
     "grade_id": "cell-d9e6599459b0ba4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 8: Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "727632a65295f42dc26bb64eca4071e5",
     "grade": false,
     "grade_id": "cell-917ded279d27040b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Saturday, January 09, 2021**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0820eb3199d1e83486b4f8a98a597ce5",
     "grade": false,
     "grade_id": "cell-433af82c3ad3533b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Redundancy and compression [4 Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c332f8b82de7fc36b569f896713fa9c",
     "grade": false,
     "grade_id": "cell-7d7b2b8f0c5697bf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain in your own words the different types of redundancy mentioned on (CV-10 slide 3). How can you check for each of these types of redundancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2138c7277ac16dd72dd117d1aaa6e26",
     "grade": true,
     "grade_id": "cell-daa37d273c908041",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "- **coding redundancy**:\n",
    "    - coding scheme for the gray values encodes more values than necessary (part of the image)\n",
    "    - check by computing entropy (information content)\n",
    "- **interpixel redundancy**:\n",
    "    - neighboring pixels have identical gray / color values\n",
    "    - check by looking for identical gray /color values of neighboring pixels (homogeneous regions)\n",
    "- **psychovisual redundancy**:\n",
    "    - information that is not needed for recognition (by humans)\n",
    "    - check e.g. by comparing pixel intensity range with recognizable levels of intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "654d62de956e3600482d89dba56efa3d",
     "grade": false,
     "grade_id": "cell-c09be7e5cf4f162a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Explain the differences between lossless and lossy compression. Name examples for both of them. Sketch application scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d746e79f554c31b0c1f6f39607735ab8",
     "grade": true,
     "grade_id": "cell-12e5d17cb8da0bbb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "- **lossless**: Only removes truly redundant information (not losing anything).\n",
    "    - example: Compression based on coding- or interpixel redundancy (redundancy of the data)\n",
    "        - e.g. Huffman coding or RLE\n",
    "- **lossy**: If lossless compression is not sufficient, the information to be lost should be chosen in a way that does not cause any problems such as worse recognizability.\n",
    "    - example: Compression based on psychovisual redundancy (redundancy of the information)\n",
    "        - e.g. orthogonal transformations to divide data into psychovisually important / unimportant parts + subsequent compression\n",
    "\n",
    "Therefore, lossy compression is not only removing true redundancies, but also information that is not that important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce7effa3c3630ce6b809b531bf4d1e82",
     "grade": false,
     "grade_id": "cell-b41e6675c5265cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Entropy based compression [8 Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "812f16d9f436d98a208fa99e8b6e0517",
     "grade": false,
     "grade_id": "cell-b873e13e731cb2d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of Huffman coding. What is the maximal compression factor that can be achieved for a given image? Load an image and compute that value (you may use `dolly.png` as an example. Make sure to load as 8-bit gray scale image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a0851466cec44d43c5a9c84a7869a95",
     "grade": true,
     "grade_id": "cell-e01b9937463ce6ef",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Huffman-Coding Steps**\n",
    "- compute normalized histogram\n",
    "- order symbols according to probability\n",
    "- generate tree\n",
    "    - merge the two least likely symbols\n",
    "    - repeat until only two symbols remain\n",
    "- start from the back and generate a prefix-free code according to the probability\n",
    "\n",
    "The result is a coding scheme that assigns the shortest codes to the most likely signals and the longest codes to the least likely ones,\n",
    "thereby removing the coding redundancy.  \n",
    "The theoretical maximum compression factor is given by $\\frac{\\#bits}{entropy}$, which intuitively makes sense, because if the entropy is high ('chaotic' image ~uniformly distributed gray values), there is not much to be compressed (low compression factor). If, on the other hand, the entropy is low (only a few different gray values), there is a lot to be compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e6c0bdb2601445ad2ba8e46e21246cf",
     "grade": true,
     "grade_id": "cell-81cfc72a8e7f4516",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imageio.imread('images/dolly.png', pilmode='L')\n",
    "# test\n",
    "#img = np.array([[1, 1, 2, 2], [2, 3, 2, 3], [2, 2, 2, 2], [3, 2, 3, 3]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def entropy(img):\n",
    "    \"\"\"\n",
    "    Compute the entropy for a given image.\n",
    "    Args:\n",
    "        img (ndarray): The grayscale image to compute the entropy for.\n",
    "    \n",
    "    Returns:\n",
    "        img_entropy (float): The entropy of the image. \n",
    "    \"\"\"\n",
    "    hist = img.flatten()\n",
    "    entropy = 0\n",
    "    for i in range(256):\n",
    "        counter = np.count_nonzero(hist == i)\n",
    "        prob = counter / (img.shape[0] * img.shape[1])\n",
    "        if prob > 0.0:\n",
    "            entropy += prob * np.log2(prob)\n",
    "    return -entropy\n",
    "\n",
    "comp_fac = (8 / entropy(img))\n",
    "print('theoretical maximum compression factor:', comp_fac)\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d05e8b2e0db92ff9033b6b45453a664",
     "grade": false,
     "grade_id": "cell-f199e258bf6cebb2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Now compute the relative frequencies (normalized histogram) of the image and generate an (approximately) balanced tree, as described in (CV-10 slide 6). *Hint:* you may use Python tuples as building blocks of a tree. Every non-leaf node is a pair `(left, right)` where `left` and `right` are the left and right subtrees, respectively (of course you are free to choose another approach if you prefer to do so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e04798af5971512ccd26572483a0145",
     "grade": true,
     "grade_id": "cell-516481ae59437cd2d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from sortedcontainers import SortedList  # conda install -c conda-forge sortedcontainers\n",
    "from heapq import *\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, frequency, subtree):\n",
    "        self.frequency = frequency\n",
    "        self.subtree = subtree\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"freq: \" + str(self.frequency) + \", sub: \" + str(self.subtree)\n",
    "\n",
    "# first compute a histogram --> count how often each gray value is used\n",
    "b, bins, patches = plt.hist(img.flatten(), 256, (0, 255))\n",
    "norm_hist = (b - np.min(b)) / (np.max(b) - np.min(b))\n",
    "\n",
    "# nodes is a sorted list of sub-trees, each annotated by its cummulative relative frequency,\n",
    "# i.e. each list item is a pair (frequency, subtree) - lowest frequencies come first\n",
    "nodes = SortedList([Node(frequency, i) for i, frequency in enumerate(norm_hist)], key=lambda x: x.frequency)\n",
    "\n",
    "tree = list(nodes)\n",
    "heapify(tree)\n",
    "\n",
    "while len(tree) > 1:\n",
    "    left = heappop(tree)\n",
    "    right = heappop(tree)\n",
    "    heappush(tree, Node(left.frequency + right.frequency, (left, right)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea8a920cf219b1d7b4a61922ac98fd9a",
     "grade": false,
     "grade_id": "cell-f0017a3bfc8c198b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Now create a prefix free code from this tree, by traversing it following the idea sketched in (CV-10 slide 7). *Hint:* if you used the tuple representation recommended in (b), you can use `isinstance(node, tuple)` to check if `node` is an inner node or a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e0d28c46c083c255787a70905389dae",
     "grade": true,
     "grade_id": "cell-4c18bcf5c721980ba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "codes = ['' for char in range(256)]\n",
    "\n",
    "# recursively traverse the tree\n",
    "def assign_codes(codes, tree, prefix=''):\n",
    "    if isinstance(tree.subtree, tuple):\n",
    "        assign_codes(codes, tree.subtree[0], prefix + '0')\n",
    "        assign_codes(codes, tree.subtree[1], prefix + '1')\n",
    "    else:\n",
    "        codes[tree.subtree] = prefix\n",
    "\n",
    "assign_codes(codes, tree[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "339b8d8e39dc366ab640174745f19dde",
     "grade": false,
     "grade_id": "cell-be6782bbc1933025",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Compute the compression ratio that you can achieve with that code. Compare this with the maximal value computed in part (a). Explain your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "576bfc366512d561564f99430963e328",
     "grade": true,
     "grade_id": "cell-ae3307c0f07aea91",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "required_memory = 0\n",
    "constant_mem = 0\n",
    "for value, code in enumerate(codes):\n",
    "    required_memory += len(code) * norm_hist[value]\n",
    "    constant_mem += 8 * norm_hist[value]\n",
    "\n",
    "print('required memory: {:.2f}'.format(required_memory))\n",
    "print('memory for constant code length: {:.2f}'.format(constant_mem))\n",
    "print(\"compression ratio:\", (constant_mem) / required_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4497d781a6fb943645b406bf0d6998bd",
     "grade": true,
     "grade_id": "cell-67f20617995a24ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The maximum compression factor computed in (a) was $\\approx 1.077$, so we end up with a compression that is pretty close to the theoretical optimum.  \n",
    "The entropy is a measure of the smallest codeword length that is theoretically possible for the given symbols. Therefore, Huffman coding performs very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1939385cb1d04435d67a8a66e1b0dc93",
     "grade": false,
     "grade_id": "cell-a7ec0ae03a0ae285",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Run length encoding [8 Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aa242cd396f69a1119bfc505df61da2",
     "grade": false,
     "grade_id": "cell-415382376019ed22",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of *run length encoding*. What are advantages and disadvantages? In what situations should it be applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7418394f74feb3a9022b7b887571f7",
     "grade": true,
     "grade_id": "cell-7ac3249713d0b868",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "RLE removes redundancy caused from identical gray / color values of neighboring pixels.  \n",
    "The image is coded line by line and segments of lines with identical gray or color value are coded in the form $(number, value)$.\n",
    "\n",
    "**Disadvantages**:\n",
    "- in the worst case, the amount of data is doubled (just each pixel together with the number $1$)\n",
    "- inefficient for noisy images because the runs are very short\n",
    "\n",
    "**Advantages**:\n",
    "- is able to reduce redundancy by grouping homogeneous areas (if there are any)\n",
    "\n",
    "It obviously should be applied for images with large (truly) homogeneous areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68f99c14103f4ecaf5c251272dda7c27",
     "grade": false,
     "grade_id": "cell-e8eb7016299142a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Analyze the run lengths in a gray scale image (8 bit = 256 gray values) by counting the number of runs and the average run length and displaying a histogram of the run lengths. What do you observe? Can you benefit from run length encoding here? (you may use `dolly.png` as an example again, but you may also experiment with other images. Make sure to load it as 8-bit gray scale image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0292a1cddd04b8796ba034c7ef197fbf",
     "grade": true,
     "grade_id": "cell-c61791fc64b45bcf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imageio.imread('images/dolly.png', pilmode='L')\n",
    "img_2 = imageio.imread('images/8bitmario.jpeg', pilmode='L')\n",
    "\n",
    "# slide example\n",
    "# img = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
    "# img  = np.array([[1, 1, 1, 2, 3, 3, 4, 4, 4, 4], [1, 1, 2, 2, 3, 4, 4, 4, 4, 4], [1, 2, 3, 4, 4, 4, 5, 5, 5, 5]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def plot_run_len_hist(runs):\n",
    "    bins = np.unique(runs)\n",
    "    hist = [runs.count(b) for b in bins]\n",
    "    plt.bar(bins, hist)\n",
    "\n",
    "def get_runs(img):\n",
    "    runs = []\n",
    "    for line in img:\n",
    "        cnt = 0\n",
    "        val_before = -1\n",
    "        line_runs = []\n",
    "        for val in line:\n",
    "            if val != val_before:\n",
    "                if val_before != -1:\n",
    "                    line_runs.append((cnt, val_before))\n",
    "                cnt = 1\n",
    "                val_before = val\n",
    "            else:\n",
    "                cnt += 1\n",
    "        if cnt != 1:\n",
    "            line_runs.append((cnt, val_before))\n",
    "        runs.append(line_runs)\n",
    "    return runs\n",
    "\n",
    "def get_num_of_runs(img):\n",
    "    runs = get_runs(img)\n",
    "    return np.sum([len(line) for line in runs])\n",
    "\n",
    "def get_avg_run_len(img):\n",
    "    runs = get_runs(img)\n",
    "    return np.average([run[0] for line in runs for run in line])\n",
    "\n",
    "print('num of runs (original img):', get_num_of_runs(img))\n",
    "print('avg run length (original img):', get_avg_run_len(img))\n",
    "print('num of runs (mario img):', get_num_of_runs(img_2))\n",
    "print('avg run length (mario img):', get_avg_run_len(img_2))\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.gray()\n",
    "\n",
    "runs = get_runs(img)\n",
    "plt.subplot(1, 4, 1); plt.title('original image'); plt.imshow(img)\n",
    "plt.subplot(1, 4, 2); plt.title('run length histogram')\n",
    "plot_run_len_hist([run[0] for line in runs for run in line])\n",
    "\n",
    "runs = get_runs(img_2)\n",
    "plt.subplot(1, 4, 3); plt.title('mario image'); plt.imshow(img_2)\n",
    "plt.subplot(1, 4, 4); plt.title('run length histogram')\n",
    "plot_run_len_hist([run[0] for line in runs for run in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "923b05a8ff6d1bfff9d9f23d86d41bd5",
     "grade": true,
     "grade_id": "cell-647dd8ad7cd04460",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "We cannot really benefit from run length encoding for `dolly.png` since it has an avg run length of $1.12$, which means that almost  \n",
    "every run has a length of $1$ and therefore the amount of data is roughly doubled, which is exactly the opposite of what one wants to achieve when using it.  \n",
    "The problem with the image is that it does not contain large (truly) homogenous areas. There is noise and even the homogenous areas do not contain exactly the same gray values.\n",
    "\n",
    "It's a lot better for the mario image that contains large truly homogenous areas and leads to a avg run length of $127.81$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "797b9b7a4bd7d231cc83bc9487269125",
     "grade": false,
     "grade_id": "cell-1c1500aa256b9686",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Now consider the individual bit planes. First display the bit planes as in (CV-10 slide 10ff). What do you observe? Apply your analysis from part (b) to each bitplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "444057e0d98cefebaeea036c72159bf4",
     "grade": true,
     "grade_id": "cell-1decc44824d08571",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imageio.imread('images/dolly.png', pilmode='L')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "bin_rep_img = [np.binary_repr(img[i][j], width=8) for i in range(img.shape[0]) for j in range(img.shape[1])]\n",
    "\n",
    "def get_bit_plane(bit):\n",
    "    return (np.array([int(i[bit]) for i in bin_rep_img], dtype=np.uint8) * (2 ** (7 - bit))).reshape(img.shape)\n",
    "\n",
    "def get_run_counts(runs):\n",
    "    return [run[0] for line in runs for run in line]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.gray()\n",
    "plt.subplot(2, 4, 1); plt.title('1st bit'); plt.imshow(get_bit_plane(0))\n",
    "plt.subplot(2, 4, 2); plt.title('2nd bit'); plt.imshow(get_bit_plane(1))\n",
    "plt.subplot(2, 4, 3); plt.title('3rd bit'); plt.imshow(get_bit_plane(2))\n",
    "plt.subplot(2, 4, 4); plt.title('4th bit'); plt.imshow(get_bit_plane(3))\n",
    "plt.subplot(2, 4, 5); plt.title('5th bit'); plt.imshow(get_bit_plane(4))\n",
    "plt.subplot(2, 4, 6); plt.title('6th bit'); plt.imshow(get_bit_plane(5))\n",
    "plt.subplot(2, 4, 7); plt.title('7th bit'); plt.imshow(get_bit_plane(6))\n",
    "plt.subplot(2, 4, 8); plt.title('8th bit'); plt.imshow(get_bit_plane(7))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.gray()\n",
    "plt.subplot(2, 4, 1); plt.title('RLE 1st bit'); runs = get_runs(get_bit_plane(0)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 2); plt.title('RLE 2nd bit'); runs = get_runs(get_bit_plane(1)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 3); plt.title('RLE 3rd bit'); runs = get_runs(get_bit_plane(2)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 4); plt.title('RLE 4th bit'); runs = get_runs(get_bit_plane(3)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 5); plt.title('RLE 5th bit'); runs = get_runs(get_bit_plane(4)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 6); plt.title('RLE 6th bit'); runs = get_runs(get_bit_plane(5)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 7); plt.title('RLE 7th bit'); runs = get_runs(get_bit_plane(6)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 8); plt.title('RLE 8th bit'); runs = get_runs(get_bit_plane(7)); plot_run_len_hist(get_run_counts(runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cbb4a3ee9d6b2d30b0acf274fcf0b45",
     "grade": true,
     "grade_id": "cell-ac74097966cc4cfd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "It works very well for the first few bit planes and gets increasingly worse towards the least significant bit plane.  \n",
    "That's not surprising since the first bit plane only distinguishes pixels from the lower and upper half of the range of possible gray values,  \n",
    "which means that small changes in the gray values between neighboring pixels still belong to the same run here.  \n",
    "In contrast, on the least significant bit plane, we again have a lot of change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c21f233d0d0ffeefe36a456fad68575",
     "grade": false,
     "grade_id": "cell-902776d0877d8cbf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Explain the idea of the *Gray code*. Why is it better suited for run length encoding? Compute a Gray code for a 256 bit image and recode the image `dolly.png`. Then analyze the run lengths of the individual bit planes of the recoded image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe349ce812e79aac007dd0e015cbb444",
     "grade": true,
     "grade_id": "cell-3aa28a8b5bfa9810",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Gray code is an ordering of the binary system such that two successive values differ in only one bit (Hamming distance $1$).  \n",
    "The gray code image does lead to an improvement, although it's not that big.  \n",
    "The enhancement results from the fact that successive number differ in only one bit, which means that only one bit plane is disturbed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "373f5dda43b52882cb9c0974f51e1584",
     "grade": true,
     "grade_id": "cell-3e854b9e4c5f7eb0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imageio.imread('images/dolly.png', pilmode='L')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "bin_rep_img = [np.binary_repr(img[i][j], width=8) for i in range(img.shape[0]) for j in range(img.shape[1])]\n",
    "\n",
    "# Binary to Gray conversion : \n",
    "#   - the most significant bit of the gray code is always equal to the MSB of the given binary code\n",
    "#   - other bits of the output gray code can be obtained by XORing binary code bit at that index and previous index\n",
    "\n",
    "def bin_to_gray(pixel):\n",
    "    gray = \"\"\n",
    "    # MSB of gray code is same\n",
    "    gray += pixel[0]\n",
    "    for i in range(1, len(pixel)):\n",
    "        # XOR of previous bit with current bit\n",
    "        gray += str(int(pixel[i - 1]) ^ int(pixel[i]))\n",
    "    return gray\n",
    "\n",
    "gray_code_img = [bin_to_gray(pixel) for pixel in bin_rep_img]\n",
    "bin_rep_img = gray_code_img\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.gray()\n",
    "plt.subplot(2, 4, 1); plt.title('1st bit'); plt.imshow(get_bit_plane(0))\n",
    "plt.subplot(2, 4, 2); plt.title('2nd bit'); plt.imshow(get_bit_plane(1))\n",
    "plt.subplot(2, 4, 3); plt.title('3rd bit'); plt.imshow(get_bit_plane(2))\n",
    "plt.subplot(2, 4, 4); plt.title('4th bit'); plt.imshow(get_bit_plane(3))\n",
    "plt.subplot(2, 4, 5); plt.title('5th bit'); plt.imshow(get_bit_plane(4))\n",
    "plt.subplot(2, 4, 6); plt.title('6th bit'); plt.imshow(get_bit_plane(5))\n",
    "plt.subplot(2, 4, 7); plt.title('7th bit'); plt.imshow(get_bit_plane(6))\n",
    "plt.subplot(2, 4, 8); plt.title('8th bit'); plt.imshow(get_bit_plane(7))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.gray()\n",
    "plt.subplot(2, 4, 1); plt.title('RLE 1st bit'); runs = get_runs(get_bit_plane(0)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 2); plt.title('RLE 2nd bit'); runs = get_runs(get_bit_plane(1)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 3); plt.title('RLE 3rd bit'); runs = get_runs(get_bit_plane(2)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 4); plt.title('RLE 4th bit'); runs = get_runs(get_bit_plane(3)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 5); plt.title('RLE 5th bit'); runs = get_runs(get_bit_plane(4)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 6); plt.title('RLE 6th bit'); runs = get_runs(get_bit_plane(5)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 7); plt.title('RLE 7th bit'); runs = get_runs(get_bit_plane(6)); plot_run_len_hist(get_run_counts(runs))\n",
    "plt.subplot(2, 4, 8); plt.title('RLE 8th bit'); runs = get_runs(get_bit_plane(7)); plot_run_len_hist(get_run_counts(runs))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('cv': conda)",
   "language": "python",
   "name": "python38664bitcvcondace24c6b5e63f40158ccc45b6baeafab5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}